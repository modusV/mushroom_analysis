{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                " # Safe to eat or deadly poisonous?\n",
                " ### An analysis on mushroom classification by Lorenzo Santolini\n",
                "\n",
                " ### Index:\n",
                "   1. [Introduction](#introduction)\n",
                "   2. [Dataset Analysis](#dsl)\n",
                "   3. [Preprocessing](#preprocessing)\n",
                "   4. [Principal Component Analysis](#pca)\n",
                "   5. [Classification](#classification)\n",
                "   6. [Conclusions](#conclusions)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " This is a little code to import automatically the dataset into google colab.\n",
                " Provide your kaggle's API key (profile section) when file requested\n",
                " ### Code snippet for google colab"
            ],
            "metadata": {}
        },
        {
            "source": [
                "# Little code snippet to import on Google Colab the dataset\n",
                "'''\n",
                "!pip install -U -q kaggle\n",
                "!mkdir -p ~/.kaggle\n",
                "\n",
                "# Insert here your kaggle API key\n",
                "from google.colab import files\n",
                "files.upload()\n",
                "\n",
                "!cp kaggle.json ~/.kaggle/\n",
                "!chmod 600 /root/.kaggle/kaggle.json\n",
                "!kaggle datasets download -d uciml/mushroom-classification\n",
                "!unzip mushroom-classification.zip\n",
                "!ls\n",
                "'''\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### Constants",
                ""
            ],
            "metadata": {}
        },
        {
            "source": [
                "# Define all the constants that will be used\n",
                "\n",
                "PLOTLY_COLORS = ['#140DFF', '#FF0DE2']\n",
                "COLOR_PALETTE = ['#140DFF', '#FF0DE2', '#CAFFD0', '#C9E4E7', '#B4A0E5', '#904C77']\n",
                "COLORSCALE_HEATMAP = [         [0.0, 'rgb(70,0,252)'], \n",
                "                [0.1111111111111111, 'rgb(78,0,252)'], \n",
                "                [0.2222222222222222, 'rgb(90,0,252)'], \n",
                "                [0.3333333333333333, 'rgb(110,0,248)'], \n",
                "                [0.4444444444444444, 'rgb(130,0,238)'], \n",
                "                [0.5555555555555556, 'rgb(145,0,228)'], \n",
                "                [0.6666666666666666, 'rgb(166,0,218)'], \n",
                "                [0.7777777777777778, 'rgb(187,0,213)'], \n",
                "                [0.8888888888888888, 'rgb(200,0,202)'], \n",
                "                               [1.0, 'rgb(210,0,191)']]\n",
                "PLOTLY_OPACITY = 0.7\n",
                "RANDOM_SEED = 11\n",
                "\n",
                "LOGISTIC_REGRESSION_PARAMS = [{\n",
                "    'clf__solver': ['liblinear'],  # best for small datasets\n",
                "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], # smaller value, stronger regularization, like svm\n",
                "    'clf__penalty': ['l2', 'l1']\n",
                "},\n",
                "{\n",
                "    'clf__solver': ['newton-cg', 'lbfgs'], \n",
                "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
                "    'clf__penalty': ['l2'] # `newton-cg` and `lbfgs` accept only l2\n",
                "}]\n",
                "\n",
                "SVM_PARAMS = [\n",
                "{\n",
                "    'clf__kernel': ['linear'],\n",
                "    'clf__C': [0.1, 1, 10, 100],\n",
                "}, \n",
                "{\n",
                "    'clf__kernel': ['rbf'],\n",
                "    'clf__C': [0.1, 1, 10, 100],\n",
                "    'clf__gamma': [0.1, 1, 10, 100],\n",
                "}]\n",
                "\n",
                "RANDOM_FOREST_PARAMS = {\n",
                "    'clf__max_depth': [50, 75, 100],\n",
                "    'clf__max_features': [\"sqrt\", \"log2\"], # sqrt is the same as auto\n",
                "    'clf__criterion': ['gini', 'entropy'],\n",
                "    'clf__n_estimators': [100, 300, 500]\n",
                "}\n",
                "\n",
                "KNN_PARAMS = {\n",
                "    'clf__n_neighbors': [2, 3, 5, 15, 25],\n",
                "    'clf__weights': ['uniform', 'distance'],\n",
                "    'clf__p': [1, 2, 10]\n",
                "}\n",
                "\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " <a id='introduction'></a>\n",
                "# Introduction\n",
                " This dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom. Each species is identified as edible or poisonous. Rows are composed by 23 different fields, each one of them identifying a specific charateristic:\n",
                " - `Class`: poisonous=p, edible=e\n",
                " - `Cap-surface`: fibrous=f, grooves=g, scaly=y, smooth=s\n",
                " - `Cap-shape`: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s\n",
                " - `Cap-color`: brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y\n",
                " - `Bruises`: bruises=t, no=f\n",
                " - `Odor`: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s\n",
                " - `Gill-attachment`: attached=a, descending=d, free=f, notched=n\n",
                " - `Gill-spacing`: close=c, crowded=w, distant=d\n",
                " - `Gill-size`: broad=b, narrow=n\n",
                " - `Gill-color`: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u,red=e, white=w, yellow=y\n",
                " - `Stalk-shape`: enlarging=e, tapering=t\n",
                " - `Stalk-root`: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=?\n",
                " - `Stalk-surface-above-ring`: fibrous=f, scaly=y, silky=k, smooth=s\n",
                " - `Stalk-surface-below-ring`: fibrous=f, scaly=y, silky=k, smooth=s\n",
                " - `Stalk-color-above-ring`: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y\n",
                " - `Stalk-color-below-ring`: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y\n",
                " - `Veil-type`: partial=p, universal=u\n",
                " - `Veil-color`: brown=n, orange=o, white=w, yellow=y\n",
                " - `Ring-number`: none=n, one=o, two=t\n",
                " - `Ring-type`: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z\n",
                " - `Spore-print-color`: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y\n",
                " - `Population`: abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y\n",
                " - `Habitat`: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d\n",
                "\n",
                " This analysis was conducted in *Python 3.7.1* using Jupyter Notebook allows you to combine code,\n",
                " comments, multimedia, and visualizations in an interactive document — called a notebook,\n",
                " naturally — that can be shared, re-used, and re-worked.\n",
                "\n",
                " In addition, the following packages were used:\n",
                " - sklearn\n",
                " - pandas\n",
                " - numpy\n",
                " - plotly\n",
                " - scipy\n",
                " - prettytable\n",
                " - imblearn"
            ],
            "metadata": {}
        },
        {
            "source": [
                "# Import all the libraries\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
                "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV, learning_curve, cross_val_score\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import log_loss, confusion_matrix, roc_curve, auc, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.utils import shuffle\n",
                "\n",
                "import plotly\n",
                "import plotly.plotly as py\n",
                "from plotly.plotly import plot, iplot\n",
                "import plotly.graph_objs as go\n",
                "import plotly.figure_factory as ff\n",
                "\n",
                "from scipy.cluster import hierarchy as hc\n",
                "import scipy.spatial as scs\n",
                "\n",
                "from imblearn.pipeline import make_pipeline, Pipeline\n",
                "\n",
                "import warnings\n",
                "from collections import defaultdict\n",
                "from prettytable import PrettyTable\n",
                "from functools import wraps\n",
                "import time\n",
                "\n",
                "plotly.tools.set_credentials_file(username='XXXXXXX', api_key='XXXXXXXXXXXXX')\n",
                "warnings.filterwarnings(\"ignore\")"
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "\n",
                "# Wrapper to calculate functions speed\n",
                "\n",
                "def watcher(func):\n",
                "    \"\"\"\n",
                "    Shows how much time it\n",
                "    takes to execute function\n",
                "    \"\"\"\n",
                "    @wraps(func)\n",
                "    def wrapper(*args, **kwargs):\n",
                "        start = time.perf_counter()\n",
                "        result = func(*args, **kwargs)\n",
                "        end = time.perf_counter()\n",
                "        print(f\" ===> took {end-start} seconds\")\n",
                "        return result\n",
                "    return wrapper\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " <a id='dsl'></a>\n",
                "# Dataset load and overall view\n",
                " Let's start importing the data:"
            ],
            "metadata": {}
        },
        {
            "source": [
                "# Load the dataset\n",
                "dataset = pd.read_csv(\"./Input/mushrooms.csv\")\n",
                "# dataset = pd.read_csv(\"./mushrooms.csv\")\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "# Shape of the dataset\n",
                "print(\"The dataset has %d rows and %d columns.\" % dataset.shape)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " We will look now at the dataset to understand what are the different fields and their types:"
            ],
            "metadata": {}
        },
        {
            "source": [
                "# Count number of classes for classification\n",
                "print(f\"There are {dataset['class'].unique().size} different classes:\"\n",
                "      f\"\\n {dataset['class'].unique().tolist()}\")\n",
                "\n",
                "# Count number of unique data for every column\n",
                "print(f\"Unique values for every field: \\n{dataset.nunique()}\")\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " We can notice that `veil-type` has just one value, therefore that column is useless\n",
                " for our analysis. We will remove it later.\n",
                "\n",
                " We can now look deeper inside the dataset. Thanks to the pandas library, we can\n",
                " see all the fields of the dataset with the respective values."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " <a id='preprocessing'></a>\n",
                "# Preprocessing\n",
                " Before starting the classification phase, we need to preprocess the dataset, in\n",
                " such a way that our classifiers will score with more accuracy and reliability.\n",
                " This is the most important step, if data are messy the classification will perform\n",
                " poorly.\n",
                " The steps that we will go trough are:\n",
                "\n",
                " 1. Check data types\n",
                " 2. Remove not significat columns, if any\n",
                " 3. Remove null values, if any\n",
                " 4. Encode string values\n",
                " 5. Check class distribution and, if classes are unbalanced, apply balancing techniques\n",
                " 6. Check data distribution using of bar graphs and box plots\n",
                " 7. Analyze correlation matrix to understand which fields are more important to classify our samples\n",
                " 8. Divide the dataset in classes array and unclassified samples\n",
                " 9. Scale our data, in such a way to center and standardize them"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### 1 - Check data types"
            ],
            "metadata": {}
        },
        {
            "source": [
                "# See data types \n",
                "print(\"Data types:\")\n",
                "dataset.head(5)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " From the above snippet we can notice that the fields are all string values;\n",
                " converting them to numeric values will make our analysis much easier. We will use a\n",
                " library called `LabelEncoder`. It allows us with a few line of code to create a mapping\n",
                " of every value in each field and transform the data in this way. We can go back to the\n",
                " original mapping simply using the `inverse_transform` function.\n",
                "\n",
                " Before this step though, we will firstly remove all the useless columns, in this\n",
                " case just `veil-type`."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### 2 - Remove any not significant column\n",
                " Now we will remove all the fields that do not add any information to our analysis,\n",
                " specifically, the fields that contain only one value (zero variance)."
            ],
            "metadata": {}
        },
        {
            "source": [
                "n_columns_original = len(dataset.columns)\n",
                "to_drop = [col for col in dataset.columns if dataset[col].nunique() == 1]\n",
                "dataset.drop(to_drop, axis=1, inplace=True)\n",
                "\n",
                "for d in to_drop:\n",
                "    print(str(d) + \" \", end=\"\")\n",
                "print(\"have been removed because they have zero variance\")\n",
                "print(f\"{n_columns_original - len(dataset.columns)} not significant columns have been removed\")\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " As we can notice, only one field was removed."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### 3 - Handling missing values\n",
                " When we find any missing value in a dataset, there are different\n",
                " approaches that can be considered:\n",
                "\n",
                " 1. Delete all rows containing a missing value\n",
                " 2. Substitute with a constant value that has meaning within the domain, such as 0, distinct from all other values.\n",
                " 3. Substitute with a value from another randomly selected record.\n",
                " 4. Substitute with mean, median or mode value for the column.\n",
                " 5. Substitute with a value estimated by another predictive model.\n",
                "\n",
                "\n",
                " We will approach this problem using the first and the second techniques:\n",
                "\n",
                " 1. We will create a parallel dataset in which all the rows containing a missing\n",
                " value will be dropped, classifying\n",
                " them as incomplete samples. This may cause a large decrement in the dataset size\n",
                " but is the only way\n",
                " to be sure that we are not going to influence our classification algorithm in any\n",
                " way.\n",
                "\n",
                " 2. It is evident from the `dataset.head()` function that our fileds are composed\n",
                " by all string values.\n",
                " Given the fact that we would need to translate in any case every field to a\n",
                " numeric one, to better display\n",
                " them in graphs, a simple approach is to keep the missing data as a peculiar number\n",
                " different from the others,\n",
                " and simply apply the transformation as they were present.\n",
                "\n",
                "\n",
                " In any case, let's start counting how many null/missing values we will find."
            ],
            "metadata": {}
        },
        {
            "source": [
                "# Check if any field is null\n",
                "if dataset.isnull().any().any():\n",
                "    print(\"There are some null values\")\n",
                "else:\n",
                "    print(\"There are no null values\")"
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " It may seem that we have no missing value from the previous analysis... Great!\n",
                "\n",
                " But wait a minute ... If we look better,from the data description we can notice that in the field\n",
                " `stalk-root` there are some missing values, marked with the question mark; let's count how many of them there are:"
            ],
            "metadata": {}
        },
        {
            "source": [
                "print(\"There are \" + str((dataset['stalk-root'] == \"?\").sum()) + \" missing values in stalk-root column\")\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " More than 25% of our samples is incomplete. Dropping all those rows may lead to a shortage of samples.\n",
                " This is why we will use the two approaches and see which one performs better.\n",
                " In any case, we will replace those value with another character.\n",
                " We will use the character `m` to indicate a missing value"
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "dataset['stalk-root'] = dataset['stalk-root'].replace({\"?\": \"m\"})\n",
                "print(dataset['stalk-root'].unique())\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### 4 - Encode string values\n",
                " As already said, we need to encode all the string values into integers, in such a way to continue our analysis in a more easy way."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "def encode_values(dataset):\n",
                "    \"\"\"\n",
                "    Encode string values of a dataset using numbers\n",
                "\n",
                "    :param (array of arrays) dataset: Input dataset to encode\n",
                "    \"\"\"\n",
                "    mapping = {}  \n",
                "    d = dataset.copy()\n",
                "    labelEncoder = LabelEncoder()\n",
                "    for column in dataset.columns:\n",
                "        labelEncoder.fit(dataset[column])\n",
                "        mapping[column] = dict(zip(labelEncoder.classes_, labelEncoder.transform(labelEncoder.classes_)))\n",
                "        d[column] = labelEncoder.transform(dataset[column])\n",
                "        \n",
                "    return d, labelEncoder, mapping\n",
                "\n",
                "def print_encoding(mapping):\n",
                "    \"\"\"\n",
                "    Prints a table with the key-value corrispondence of an encoding\n",
                "\n",
                "    :param (dict) mapping: dictionary containing value before and after encoding\n",
                "    \"\"\"\n",
                "    t = PrettyTable()\n",
                "    rows = []\n",
                "    for key, value in mapping.items():\n",
                "        r = []\n",
                "        r.append(key)\n",
                "        for k, v in value.items():\n",
                "            r.append(k)\n",
                "        rows.append(r)\n",
                "    max = []\n",
                "    for r in rows:\n",
                "        if len(r) > len(max):\n",
                "            max = r\n",
                "\n",
                "    for r in rows:\n",
                "        r = r + ['-'] * (len(max) - len(r))\n",
                "        t.add_row(r)\n",
                "    t.field_names = [\"Columns / Values\"] + list(range(0, len(max)-1))\n",
                "    print(t)\n",
                "\n",
                "le = 0\n",
                "pre_data, l_encoder, le_mapping = encode_values(dataset)\n",
                "\n",
                "# Check mapping\n",
                "print_encoding(le_mapping)\n",
                "\n",
                "# Check new data\n",
                "pre_data.head(5)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " As we can see data have been transformed; all the strings values now are equal to integers,\n",
                " and we can see the direct corrispondence from the table above.\n",
                "\n",
                " We have encoded our dataset but, depending on the data (as in our case), label encoding may\n",
                " introduce a new problem. For example, we have encoded a set of colour names\n",
                " into numerical data. This is actually categorical data and there is no relation,\n",
                " of any kind, between the rows.\n",
                "\n",
                " The problem here is, since there are different numbers in the same column,\n",
                " the model will misunderstand the data to be in some kind of order, 0 < 1 < 2.\n",
                " But this isn’t the case at all. To overcome this problem, we use `OneHotEncoder`.\n",
                "\n",
                " What one hot encoding does is, it takes a column which has categorical data,\n",
                " which has been label encoded, and then splits the column into multiple columns.\n",
                " The numbers are replaced by 1s and 0s, depending on which column has what value.\n",
                "\n",
                " We will obtain two datasets; we will continue the analysis on the first one, but\n",
                " meanwhile we will keep this one for the classification phase, to check if we\n",
                " have an improvement with respect to the other.\n",
                "\n",
                " We also drop the column with missing value, beacuse it is not significative."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "def one_hot_encode(X_dataset):\n",
                "\n",
                "    ohc = defaultdict(OneHotEncoder)\n",
                "    d = defaultdict (LabelEncoder)\n",
                "    Xfit = X_dataset.apply(lambda x: d[x.name].fit_transform(x))\n",
                "    final = pd.DataFrame()\n",
                "\n",
                "    for i in range(len(X_dataset.columns)):\n",
                "        # Transform columns using OneHotEncoder\n",
                "        Xtemp_i = pd.DataFrame(ohc[Xfit.columns[i]].fit_transform(Xfit.iloc[:,i:i+1]).toarray())\n",
                "    \n",
                "        # Naming the columns\n",
                "        ohc_obj  = ohc[Xfit.columns[i]]\n",
                "        labelEncoder_i= d[Xfit.columns[i]]\n",
                "        Xtemp_i.columns= Xfit.columns[i]+\"-\"+labelEncoder_i.inverse_transform(ohc_obj.active_features_)\n",
                "        \n",
                "        # Take care of dummy variable trap dropping of new coulmns\n",
                "        X_ohc_i = Xtemp_i.iloc[:,1:]\n",
                "        \n",
                "        # Append columns to dataframe\n",
                "        final = pd.concat([final,X_ohc_i],axis=1)\n",
                "\n",
                "    return final\n",
                "\n",
                "pre_ohc_data = one_hot_encode(dataset.iloc[:,1:])\n",
                "pre_ohc_data.drop(['stalk-root-m'], axis=1, inplace=True)\n",
                "pre_ohc_data.head(5)\n",
                "\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " As we can see, we obtained a huge dataset, where all the fields are binary."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### 5/6 - Check class and data distribution\n",
                "In this phase, we will analyze the distribution of the data. The steps will be:\n",
                "\n",
                " 1. Check amount of samples belonging to a class or to another.\n",
                " 2. Analyze the overall distribution using box plots, a very useful tool to identify outliers and values taken by samples.\n",
                " 3. Compare the two classes distributions with the help of an histogram."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " #### 5 - Check classes distribution\n",
                " Let's see how many samples belong to the different classes"
            ],
            "metadata": {}
        },
        {
            "source": [
                "y = dataset[\"class\"].value_counts()\n",
                "print(y)\n",
                "class_dict = [\"edible\", \"poisonous\"]\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " Luckily the dataset is pretty balanced:\n",
                " We have almost the same amount of samples in a class and in the other; this simplifies the analysis because we can assign the same weight to the two classes in the classification phase.\n",
                "\n",
                " Moreover, we can notice that the classification task will be binary. Infact, data have been transformed, and now the labels are represented with a 0/1 integer value.\n",
                " Now we can look deeper into some statistical details about the dataset, using the `pre_df.describe()` command on our pandas DataFrame dataset. The output shows:\n",
                "\n",
                " - count: number of samples (rows)\n",
                " - mean: the mean of the attribute among all samples\n",
                " - std: the standard deviation of the attribute\n",
                " - min: the minimal value of the attribute\n",
                " - 25%: the lower percentile\n",
                " - 50%: the median\n",
                " - 75%: the upper percentile\n",
                " - max: the maximal value of the attribute"
            ],
            "metadata": {}
        },
        {
            "source": [
                "# Get insights on the dataset\n",
                "pre_data.describe()\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " This is the class distribution plot on a graph bar. As we already saw before the class distribution is pretty balanced.\n",
                "\n",
                " In this report, all the graphic part will use the `plotly` library.\n",
                "\n",
                " *plotly.py* is an interactive, open-source, and browser-based graphing library for Python, which allows you to create interactive plots in a few steps."
            ],
            "metadata": {}
        },
        {
            "source": [
                "data = [go.Bar(\n",
                "            x=class_dict,\n",
                "            y=y,\n",
                "            marker=dict(\n",
                "            color=PLOTLY_COLORS),\n",
                "            opacity=PLOTLY_OPACITY,\n",
                "    )]\n",
                "\n",
                "layout = go.Layout(title=\"Class distribution\",\n",
                "                   autosize=True,\n",
                "                   yaxis=dict(\n",
                "                        title='N. samples',\n",
                "                    ),\n",
                "                   )\n",
                "fig = go.Figure(data=data, layout=layout)\n",
                "py.iplot(fig, filename='distribution-bar')\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " #### 6 - Box plot\n",
                "\n",
                " At this point we can analyze the distribution of our data using a boxplot.\n",
                " A boxplot is a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”).\n",
                "\n",
                " It can tell you about your outliers and what their values are.\n",
                " It can also tell you if your data is symmetrical, how tightly your data is grouped,and if and how your data is skewed.\n",
                " The information that we can find in a box plot are:\n",
                "\n",
                " - **median** (Q2/50th Percentile): the middle value of the dataset.\n",
                " - **first quartile** (Q1/25th Percentile): the middle number between the smallest number (not the “minimum”) and the median of the dataset.\n",
                " - **third quartile** (Q3/75th Percentile): the middle value between the median and the highest value (not the “maximum”) of the dataset.\n",
                " - **interquartile range** (IQR): 25th to the 75th percentile.\n",
                " - **outliers** (shown as green circles)\n",
                " - **maximum**: Q3 + 1.5*IQR\n",
                " - **minimum**: Q1 -1.5*IQR\n",
                "\n",
                " It makes no sense showing binary or with few different values fields, so we are going to filter them before plotting."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "def create_box(type, data, col, visible=False):\n",
                "    if type == \"edible\":\n",
                "        c = PLOTLY_COLORS[0]\n",
                "    else:\n",
                "        c = PLOTLY_COLORS[1]\n",
                "    return go.Box(\n",
                "        y = data[col],\n",
                "        name = type,\n",
                "        marker=dict(color = c),\n",
                "        visible=visible,\n",
                "        opacity=PLOTLY_OPACITY,\n",
                "    )\n",
                "\n",
                "edible = pre_data[pre_data[\"class\"] == 0]\n",
                "poisonous = pre_data[pre_data[\"class\"] == 1]\n",
                "box_features = [col for col in pre_data.columns if ((col != 'class') and (dataset[col].nunique() > 5))]\n",
                "\n",
                "active_index = 0\n",
                "box_edible = [(create_box(\"edible\", edible, col, False) if i != active_index \n",
                "               else create_box(\"edible\", edible, col, True)) \n",
                "              for i, col in enumerate(box_features)]\n",
                "\n",
                "box_poisonous = [(create_box(\"poisonous\", poisonous, col, False) if i != active_index \n",
                "               else create_box(\"poisonous\", poisonous, col, True)) \n",
                "              for i, col in enumerate(box_features)]\n",
                "\n",
                "data = box_edible + box_poisonous\n",
                "n_features = len(box_features)\n",
                "steps = []\n",
                "\n",
                "for i in range(n_features):\n",
                "    step = dict(\n",
                "        method = 'restyle',  \n",
                "        args = ['visible', [False] * len(data)],\n",
                "        label = box_features[i],\n",
                "    )\n",
                "    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
                "    step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n",
                "    steps.append(step)\n",
                "    \n",
                "sliders = [dict(\n",
                "    active = active_index,\n",
                "    currentvalue = dict(\n",
                "        prefix = \"Feature: \", \n",
                "        xanchor= 'center',\n",
                "    ),\n",
                "    pad = {\"t\": 50},\n",
                "    steps = steps,\n",
                "    len=1,\n",
                ")]\n",
                "\n",
                "layout = dict(\n",
                "    sliders=sliders,\n",
                "    autosize=True,\n",
                "    yaxis=dict(\n",
                "        title='value',\n",
                "        automargin=True,\n",
                "    ),\n",
                "    legend=dict(\n",
                "        x=0,\n",
                "        y=1,\n",
                "    ),\n",
                ")\n",
                "\n",
                "fig = dict(data=data, layout=layout)\n",
                "py.iplot(fig, filename='box_slider')\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " From the boxplot above, we can see that the color and the shape of the\n",
                " cap are not an effective parameter to decide whether a mushroom is\n",
                " poisonous or edible, because their plots are very similar (same median\n",
                " and very close distribution).\n",
                " The `odor` and the `population` columns, on the other hand, are more significant;\n",
                "\n",
                " In the `odor` field, all the edible mushrooms are squeezed into a single value\n",
                " with a few outliers, while the poisonous may have all the different values."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " #### 6 - Bar graph\n",
                "\n",
                " A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent.\n",
                " With a slider we can move along the different features, to better visualize the value distributions."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "def create_bar(type, data, col, visible=False):\n",
                "    if type == \"edible\":\n",
                "        c = PLOTLY_COLORS[0]\n",
                "    else:\n",
                "        c = PLOTLY_COLORS[1]\n",
                "    return go.Histogram(\n",
                "        x = data[col],\n",
                "        name = type,\n",
                "        marker=dict(color = c),\n",
                "        visible=visible,\n",
                "        opacity=PLOTLY_OPACITY,\n",
                "    )\n",
                "\n",
                "hist_features = [col for col in pre_data.columns if (col != 'class')]\n",
                "\n",
                "active_index = 0\n",
                "hist_edible = [(create_bar(\"edible\", edible, col, False) if i != active_index \n",
                "               else create_bar(\"edible\", edible, col, True)) \n",
                "              for i, col in enumerate(hist_features)]\n",
                "\n",
                "hist_poisonous = [(create_bar(\"poisonous\", poisonous, col, False) if i != active_index \n",
                "               else create_bar(\"poisonous\", poisonous, col, True)) \n",
                "              for i, col in enumerate(hist_features)]\n",
                "\n",
                "total_data = hist_edible + hist_poisonous\n",
                "n_features = len(hist_features)\n",
                "steps = []\n",
                "\n",
                "for i in range(n_features):\n",
                "    step = dict(\n",
                "        method = 'restyle',  \n",
                "        args = ['visible', [False] * len(total_data)],\n",
                "        label = hist_features[i],\n",
                "    )\n",
                "    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
                "    step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n",
                "    steps.append(step)\n",
                "    \n",
                "sliders = [dict(\n",
                "    active = active_index,\n",
                "    currentvalue = dict(\n",
                "        prefix = \"Feature: \", \n",
                "        xanchor= 'center',\n",
                "    ),\n",
                "    pad = {\"t\": 50},\n",
                "    steps = steps,\n",
                ")]\n",
                "\n",
                "layout = dict(\n",
                "    sliders=sliders,\n",
                "    autosize=True,\n",
                "    yaxis=dict(\n",
                "        title='value',\n",
                "        automargin=True,\n",
                "    ),\n",
                "    legend=dict(\n",
                "        x=0,\n",
                "        y=1,\n",
                "    ),\n",
                "    barmode='group',\n",
                "    bargap=0.15,\n",
                "    bargroupgap=0.1\n",
                ")\n",
                "\n",
                "fig = dict(data=total_data, layout=layout)\n",
                "py.iplot(fig, filename='bar_slider')\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " As we saw from the box plot, also here we can notice that\n",
                " the `cap-shape` and the `cap-color` are not that significant to classify a sample.\n",
                " On the other hand, some fields like `odor` show a distinct\n",
                " separation of the two classes; these ones will be the ones with more\n",
                " impact on our classification algorithms.\n",
                "\n",
                " From these bar graphs we can see that our dataset is pretty well\n",
                " separated. This means that, in our classification task, we will be\n",
                " able to achieve high accuracy even with some dimensionality reduction."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### 7 - Correlation matrix\n",
                " A correlation matrix is a table showing correlation coefficients between sets of variables.\n",
                " Each random variable (Xi) in the table is correlated with each of the other values in the table (Xj).\n",
                " This allows you to see which pairs have the highest correlation. Correlation is any statistical association, though in common usage it most often refers to how close two variables are to having a linear relationship with each other.\n",
                "\n",
                " We will use the **Pearson's correlation**, which is a measure of the linear correlation between two variables X and Y. According to the Cauchy–Schwarz inequality it has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation.\n",
                "\n",
                " The coefficient for a population is computed as:\n",
                "\n",
                " $\\rho_{(X,Y)} = \\frac{cov(X,Y)}{\\sigma_X\\sigma_Y}$\n",
                "\n",
                " Where:\n",
                " - $cov$ is the covariance:\n",
                "   - $cov(X,Y) = \\frac{E[(X - \\mu_X)(Y-\\mu_Y)]}{\\sigma_X\\sigma_Y}$\n",
                "    - Where $\\mu$ is the mean value\n",
                "\n",
                " - $\\sigma_X$ is the standard deviation of X\n",
                "   - $\\sigma_X^2 = E[X^2] - [E[X]]^2$\n",
                " - $\\sigma_Y$ is the standard deviation of Y\n",
                "   - $\\sigma_Y^2 = E[Y^2] - [E[Y]]^2$"
            ],
            "metadata": {}
        },
        {
            "source": [
                "correlation_matrix = pre_data.corr(method='pearson')\n",
                "\n",
                "trace = go.Heatmap(\n",
                "    z=correlation_matrix.values.tolist(), \n",
                "    x=correlation_matrix.columns, \n",
                "    y=correlation_matrix.columns, \n",
                "    colorscale=COLORSCALE_HEATMAP,\n",
                "    opacity=0.95,\n",
                "    zmin=-1,\n",
                "    zmax=1)\n",
                "    \n",
                "\n",
                "data=[trace]\n",
                "\n",
                "layout = go.Layout(\n",
                "    title='Heatmap of columns correlation',\n",
                "    autosize=False,\n",
                "    width=850,\n",
                "    height=700,\n",
                "    yaxis=go.layout.YAxis(automargin=True),\n",
                "    xaxis=dict(tickangle=40),\n",
                "    margin=go.layout.Margin(l=0, r=200, b=200, t=80)\n",
                ")\n",
                "\n",
                "fig = go.Figure(data=data, layout=layout)\n",
                "py.iplot(fig, filename='labelled-heatmap4')\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " From the matrix, we can see that the most correlated columns to\n",
                " the `class` field are `gill-color`, `gill-size` and `bruises`.\n",
                "\n",
                " The diagonal has correlation 1 because every class has maximum correlation with itself.\n",
                "\n",
                " We can also see that `veil-color` and `gill-attachment` are highly correlated."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### 7 - Dendogram\n",
                " A **dendrogram** is a diagram representing a tree. This diagrammatic representation is frequently used in different contexts, but we will see the case representing hierarchical clustering.\n",
                "\n",
                " It illustrates the arrangement of the clusters, and its objective is to analyze if we have any duplicate features. In order to reduce the dimensionality of our dataset, we can identify and remove duplicate features according to their pairwise correlation with others.\n",
                "\n",
                " The linkage criterion determines the distance between sets of observations as a function of the pairwise distances between observations.\n",
                " We will use the between-group average linkage (UPGMA). Proximity between two clusters is the arithmetic mean of all the proximities between the objects of one, on one side, and the objects of the other, on the other side.\n",
                " The method is frequently set the default one in hierarhical clustering packages."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "names = pre_data.columns\n",
                "inverse_correlation = 1 - abs(pre_data.corr()) # This is the 'dissimilarity' method\n",
                "\n",
                "fig = ff.create_dendrogram(inverse_correlation.values, \n",
                "                           labels=names, \n",
                "                           colorscale=COLOR_PALETTE, \n",
                "                           linkagefun=lambda x: hc.linkage(x, 'average'))\n",
                "\n",
                "fig['layout'].update(dict(\n",
                "    title=\"Dendrogram of correlation among features\",\n",
                "    width=800, \n",
                "    height=600,\n",
                "    xaxis=dict(\n",
                "        title='Features',\n",
                "    ),\n",
                "    yaxis=dict(\n",
                "        title='Distance',\n",
                "        \n",
                "    ),\n",
                "))\n",
                "iplot(fig, filename='dendrogram_corr_clustering')\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " From the above graph, the closest features are `veil-color` and `gill-attachment`.\n",
                " Because that their distance is still far from zero, I choose not to remove any of the two features; moreover our dataset is pretty small, so we should not have great performance issues."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### 8/9 - Scale and divide data\n",
                " Most of the times, datasets contain features highly varying in magnitudes, units and range.\n",
                " But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem. If left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, for example between 5kg and 5000gms.\n",
                "\n",
                " The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.\n",
                " To supress this effect, we need to bring all features to the same level of magnitudes. This can be acheived by scaling.\n",
                "\n",
                " We will use the `StandardScaler`, which standardizes our data both with mean and standard deviation.\n",
                "\n",
                " The operation performed will be:\n",
                "\n",
                " $$\n",
                " x' = \\frac{x - \\mu_x}{\\sigma_x}\n",
                " $$\n",
                " After this step, we divide the dataset into an array of unclassified samples and an array of labels, to use for the classification phase.\n",
                "\n",
                " At the end, we decide to bring to the next step of our analysis two\n",
                " different datasets, plus the one that we created in the encoding phase.\n",
                "\n",
                " 1. The full dataset, where the missing values are encoded with an integer.\n",
                " 2. A reduced version of the dataset, where the rows with missing data are considered as incomplete samples and are dropped.\n",
                " 3. The full dataset, with the `stalk-root` column removed."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "def dataframe_to_array(data):\n",
                "    y_data = data['class']\n",
                "    X_data = data.drop(['class'], axis=1)\n",
                "    return X_data, y_data\n",
                "\n",
                "def scale_data(X_data):\n",
                "    \"\"\"\n",
                "    Scales data using mean and std\n",
                "\n",
                "    :param (array) pca: Array of data\n",
                "    \"\"\"\n",
                "    scaler = StandardScaler(with_mean=True, with_std=True, copy=True)\n",
                "    return scaler.fit_transform(X_data)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " Before dropping the samples containing a missing value, we will check how many of them belong to a class or to the other;",
                " we perform this step because it is possible that we unintentionally remove most of the samples from one of the two classes",
                " creating some sort of imbalance within the data.\n"
            ],
            "metadata": {}
        },
        {
            "source": [
                "edible_removed = np.sum(dataset[dataset['stalk-root'] == \"?\"]['class'] == 'e')\n",
                "poisonous_removed = np.sum(dataset[dataset['stalk-root'] == \"?\"]['class'] == 'p')\n",
                "print(\"Will be removed:\\n\"\n",
                "f\"\\t- {edible_removed} edible samples\\n\"\n",
                "f\"\\t- {poisonous_removed} poisonous samples\")\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " We can notice that our missing values are mainly in the poisonous samples. Removing them may create an\n",
                " imbalance in the number of rows in the two classes, creating issues in some classifiers (e.g. Naive Bayes). \n"
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "drop_data = pre_data[pre_data['stalk-root'] != le_mapping['stalk-root']['m']]\n",
                "data_no_stalk = pre_data.drop(['stalk-root'], axis=1)\n",
                "\n",
                "X_pre_data, y_data = dataframe_to_array(pre_data)\n",
                "X_scaled_data = scale_data(X_pre_data)\n",
                "\n",
                "X_drop_data, y_drop_data = dataframe_to_array(drop_data)\n",
                "X_scaled_drop_data = scale_data(X_drop_data)\n",
                "\n",
                "X_no_stalk, y_no_stalk = dataframe_to_array(data_no_stalk)\n",
                "X_scaled_no_stalk = scale_data(X_no_stalk)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " <a id='pca'></a>\n",
                " ## Principal component analysis\n",
                " When our data are represented by a matrix too large (the number of dimensions is too high), it is difficult to extract the most interesting features and find correlations among them; moreover the space occupied is very high. PCA is a technique that allows to achieve dimensionality reduction while preserving the most important differences among samples.\n",
                "\n",
                " This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set.\n",
                "\n",
                " Let's calculate the Principal Components and show their retained variance on a bar graph."
            ],
            "metadata": {}
        },
        {
            "source": [
                "def plot_cumulative_variance(pca):\n",
                "    \"\"\"\n",
                "    Plots cumulative variance of all PC\n",
                "\n",
                "    :param (pca object) pca: Pca object\n",
                "    \"\"\"   \n",
                "\n",
                "    tot_var = np.sum(pca.explained_variance_)\n",
                "    ex_var = [(i / tot_var) * 100 for i in sorted(pca.explained_variance_, reverse=True)]\n",
                "    cum_ex_var = np.cumsum(ex_var)\n",
                "\n",
                "    cum_var_bar = go.Bar(\n",
                "        x=list(range(1, len(cum_ex_var) + 1)), \n",
                "        y=ex_var,\n",
                "        name=\"Variance of each component\",\n",
                "        marker=dict(\n",
                "            color=PLOTLY_COLORS[0],\n",
                "        ),\n",
                "        opacity=PLOTLY_OPACITY\n",
                "        )\n",
                "\n",
                "    variance_line = go.Scatter(\n",
                "        x=list(range(1, len(cum_ex_var) + 1)),\n",
                "        y=cum_ex_var,\n",
                "        mode='lines+markers',\n",
                "        name=\"Cumulative variance\",\n",
                "        marker=dict(\n",
                "            color=PLOTLY_COLORS[1],\n",
                "        ),\n",
                "        opacity=PLOTLY_OPACITY,\n",
                "        line=dict(\n",
                "            shape='hv',\n",
                "        ))\n",
                "    data = [cum_var_bar, variance_line]\n",
                "    layout = go.Layout(\n",
                "        title='Individual and Cumulative Explained Variance',\n",
                "        autosize=True,\n",
                "        yaxis=dict(\n",
                "            title='Explained variance (%)',\n",
                "        ),\n",
                "        xaxis=dict(\n",
                "            title=\"Principal components\",\n",
                "            dtick=1,\n",
                "            rangemode='nonnegative'\n",
                "        ),\n",
                "        legend=dict(\n",
                "            x=0,\n",
                "            y=1,\n",
                "        ),\n",
                "    )\n",
                "    fig = go.Figure(data=data, layout=layout)\n",
                "    return iplot(fig, filename='basic-bar')\n",
                "\n",
                "def compress_data(X_dataset, n_components, plot_comp=False):\n",
                "    \n",
                "    \"\"\"\n",
                "    Performs pca reduction of a dataset.\n",
                "\n",
                "    :param (array of arrays) X_dataset: Dataset to reduce\n",
                "    :param (int) n_components: N components to project on \n",
                "    :param (bool) plot_comp: Plot explained variance\n",
                "\n",
                "    :returns (pandas dataframe) X_df_reduced: pandas dataframe with reduced dataset\n",
                "    :return (iplot) p: Plot, only if plot_com equals True. To plot the graph,\n",
                "                       simply call the return value.\n",
                "    \"\"\"   \n",
                "\n",
                "    pca = PCA(random_state=RANDOM_SEED)\n",
                "    projected_data = pca.fit_transform(X_dataset)\n",
                "\n",
                "    if plot_comp:\n",
                "        p = plot_cumulative_variance(pca)\n",
                "\n",
                "    pca.components_ = pca.components_[:n_components]\n",
                "    reduced_data = np.dot(projected_data, pca.components_.T)\n",
                "    X_df_reduced = pd.DataFrame(reduced_data, columns=[\"PC#%d\" % (x + 1) for x in range(n_components)])\n",
                "    if plot_comp:\n",
                "        return p, X_df_reduced\n",
                "    else:\n",
                "        return X_df_reduced\n",
                "    "
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "\n",
                "plot, X_df_reduced = compress_data(X_dataset=X_scaled_data,\n",
                "                             n_components=9,\n",
                "                             plot_comp=True)\n",
                "plot                       \n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " From the graph we can see that the first 9 components retain almost 80% of total variance,\n",
                " while last 5 not even 2%. We then choose to select first nine of them.\n",
                "\n",
                " This allows us to work on a smaller dataset achieving similar results,\n",
                " because most of the information is maintained.\n",
                "\n",
                " In the function `compress_data`, the dataset is also projected on these vectors,\n",
                " in such a way to obtain data reduction. This is a simple scalar product:"
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "X_df_drop_reduced = compress_data(X_dataset=X_scaled_drop_data,\n",
                "                                  n_components=9,\n",
                "                                  plot_comp=False)\n",
                "\n",
                "plot, X_df_ohc_reduced = compress_data(X_dataset=pre_ohc_data,\n",
                "              n_components=20,\n",
                "              plot_comp=True)\n",
                "plot"
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " We can see that on those many components, some of them can be excluded. On this dataset we will keep\n",
                " the 80% variance rule, so I decided to project data on the first 20 components."
            ],
            "metadata": {}
        },
        {
            "source": [
                "X_df_reduced.head(4)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                " We will try now to use a scatte-plot to show if a clustering algorithm applied\n",
                " on the first two principal components is able to separate the samples in two clusters.\n",
                "\n",
                " PCA tries to find combinations of features that lead to maximum separation between data points.\n",
                " What this means is that, if we had a dimension in our dataset which was the same for all members,\n",
                " then that would not be considered, alone or in combination, among the top principal components.\n",
                " Only the features that vary a lot from data point to data point form a part of the top principal components.\n",
                " As a result, the points should appear to be quite far apart from each other on the plot.\n",
                "\n",
                " The plot of the PCA clusters may not make sense in a cuple of conditions:\n",
                "\n",
                " 1. There is a lot of variance in the dataset, so the first two components\n",
                " cannot significantly represent the data\n",
                " 2. The clustering algorithm focuses on features considered unimportant by the\n",
                " PCA.\n",
                "\n",
                " Our dataset should not have that much variance, so the clustering\n",
                " algorithm should be able to decently separate data in two clusters.\n",
                ""
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "values = pre_data.values\n",
                "pca = PCA(n_components=2)\n",
                "x = pca.fit_transform(values)\n",
                "\n",
                "kmeans = KMeans(n_clusters=2, random_state=RANDOM_SEED)\n",
                "X_clustered = kmeans.fit_predict(values)\n",
                "\n",
                "print(len(np.where(X_clustered == 0)[0]))\n",
                "print(len(np.where(X_clustered == 1)[0]))\n",
                "\n",
                "c1_idx = np.where(X_clustered == 0)\n",
                "c2_idx = np.where(X_clustered == 1)\n",
                "\n",
                "p1 = go.Scatter(\n",
                "    x=np.take(x[:,0], indices=c1_idx)[0],\n",
                "    y=np.take(x[:,1], indices=c1_idx)[0],\n",
                "    mode='markers',\n",
                "    name=\"Cluster1\",\n",
                "    marker=dict(\n",
                "        color=PLOTLY_COLORS[0],\n",
                "    ),\n",
                "    opacity=PLOTLY_OPACITY)\n",
                "\n",
                "p2 = go.Scatter(\n",
                "    x=np.take(x[:,0], indices=c2_idx)[0],\n",
                "    y=np.take(x[:,1], indices=c2_idx)[0],\n",
                "    mode='markers',\n",
                "    name=\"Cluster2\",\n",
                "    marker=dict(\n",
                "        color=PLOTLY_COLORS[1],\n",
                "    ),\n",
                "    opacity=PLOTLY_OPACITY)\n",
                "    \n",
                "\n",
                "data = [p1, p2]\n",
                "\n",
                "layout = go.Layout(\n",
                "    title='Data clustered using first two components',\n",
                "    autosize=True,\n",
                "    yaxis=dict(\n",
                "        title='Second component',\n",
                "    ),\n",
                "    xaxis=dict(\n",
                "        title=\"First component\",\n",
                "        dtick=1,\n",
                "    ),\n",
                "    legend=dict(\n",
                "        x=0,\n",
                "        y=1,\n",
                "    ),\n",
                ")\n",
                "fig = go.Figure(data=data, layout=layout)\n",
                "iplot(fig, filename='clusters-scatter')"
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " As expected, using K-means we are able to separate two groups of data using the two components with maximum variance.\n",
                ""
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " <a id='classification'></a>\n",
                "# Classification\n",
                " Now we are going to explore different supervised learning classification methods, and see in the end the one that performs better.\n",
                "\n",
                " Now, before starting the classification phase, let's see what kind of pre-processed data it is better to use to achieve the best classification possible.\n",
                " Due to the fact that our dataset is pretty small, probably the dimensionality reduction using PCA is not strictly necessary, but if we can achieve a similar score using just main principal components, it is definitely better.\n",
                "\n",
                " We are going to compare results of several classification methods on the different datasets. In this way we can choose the one\n",
                " to pick for the next phase.\n",
                " The current versions of the dataset are:\n",
                "\n",
                " 1. Full dataset.\n",
                " 2. Reduced dataset by means of PCA.\n",
                " 3. Dataset with missing values removed.\n",
                " 4. Dataset with missing values removed reduced using PCA\n",
                " 5. Dataset with column containing missing values removed\n",
                " 6. Dataset encoded with OneHotEncoder\n",
                " 7. Dataset encoded with OneHotEncoder compressed using PCA\n",
                "\n",
                " Our dataset is pretty balanced, so we do not strictly need any over or under-sampling technique.\n",
                " Let's start with splitting the datasets in train and test."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X_scaled_data, y_data, test_size=0.2, random_state=RANDOM_SEED)\n",
                "X_train_pc, X_test_pc, y_train_pc, y_test_pc = train_test_split(X_df_reduced, y_data, test_size=0.2, random_state=RANDOM_SEED)\n",
                "\n",
                "X_train_drop, X_test_drop, y_train_drop, y_test_drop = train_test_split(X_scaled_drop_data, y_drop_data, test_size=0.2, random_state=RANDOM_SEED)\n",
                "X_train_pc_drop, X_test_pc_drop, y_train_pc_drop, y_test_pc_drop = train_test_split(X_df_drop_reduced, y_drop_data, test_size=0.2, random_state=RANDOM_SEED)\n",
                "X_train_no_stalk, X_test_no_stalk, y_train_no_stalk, y_test_no_stalk = train_test_split(X_scaled_no_stalk, y_no_stalk, test_size=0.2, random_state=RANDOM_SEED)\n",
                "\n",
                "X_train_ohc_pc, X_test_ohc_pc, y_train_ohc_pc, y_test_ohc_pc = train_test_split(X_df_ohc_reduced, y_data, test_size=0.2, random_state=RANDOM_SEED)\n",
                "X_train_ohc, X_test_ohc, y_train_ohc, y_test_ohc = train_test_split(pre_ohc_data, y_data, test_size=0.2, random_state=RANDOM_SEED)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " We will apply different classification models on our datasets without any tuning of the parameters. In this way we can\n",
                " see which datasets gives an overall efficient tradeoff between accuracy and time.\n",
                " In the next phase we will analyze one classifier at a time optimizing its parameters to obtain the best\n",
                " accuracy.\n",
                "\n",
                " Let's start defining the functions that we are going to use:"
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "def print_gridcv_scores(grid_search, n=5):\n",
                "    \"\"\"\n",
                "    Prints the best score achieved by a grid_search, alongside with its parametes\n",
                "\n",
                "    :param (estimator) clf: Classifier object\n",
                "    :param (int) n: Best n scores \n",
                "    \"\"\"    \n",
                "    \n",
                "    t = PrettyTable()\n",
                "\n",
                "    print(\"Best grid scores on validation set:\")\n",
                "    indexes = np.argsort(grid_search.cv_results_['mean_test_score'])[::-1][:n]\n",
                "    means = grid_search.cv_results_['mean_test_score'][indexes]\n",
                "    stds = grid_search.cv_results_['std_test_score'][indexes]\n",
                "    params = np.array(grid_search.cv_results_['params'])[indexes]\n",
                "    \n",
                "    t.field_names = ['Score'] + [f for f in params[0].keys()] \n",
                "    for mean, std, params in zip(means, stds, params):\n",
                "        row=[\"%0.3f (+/-%0.03f)\" % (mean, std * 2)] + [p for p in params.values()]\n",
                "        t.add_row(row)\n",
                "    print(t)\n",
                "               \n",
                "\n",
                "def param_tune_grid_cv(clf, params, X_train, y_train, cv, execution_time=False):\n",
                "    \"\"\"\n",
                "    Function that performs a grid search over some parameters\n",
                "\n",
                "    :param (estimator) clf: Classifier object\n",
                "    :param (dictionary) params: parameters to be tested in grid search\n",
                "    :param (array-like) X_train: List of data to be trained with\n",
                "    :param (array-like) y_train: Target relative to X for classification or regression\n",
                "    :param (cross-validation generator) cv: Determines the cross-validation splitting strategy\n",
                "    \"\"\" \n",
                "    if execution_time:\n",
                "      start = time.perf_counter()\n",
                "    pipeline = Pipeline([('clf', clf)])\n",
                "    grid_search = GridSearchCV(estimator=pipeline, \n",
                "                               param_grid=params, \n",
                "                               cv=cv, \n",
                "                               n_jobs=-1,       # Use all processors\n",
                "                               scoring='f1',    # Use f1 metric for evaluation\n",
                "                               return_train_score=True)\n",
                "    grid_search.fit(X_train, y_train)\n",
                "    if execution_time:\n",
                "      end = time.perf_counter()\n",
                "      return grid_search, \"%.4f\" % (end-start)\n",
                "    return grid_search\n",
                "   \n",
                "\n",
                "def score(clfs, datasets):\n",
                "    \"\"\"\n",
                "    Function that scores a classifier on some data\n",
                "    \n",
                "    :param (array of estimator) clf: Array of classifiers\n",
                "    :param (dictionary) params: Dictionary of test data, passed like [(X_test, y_test)]\n",
                "\n",
                "    \"\"\"  \n",
                "    scores = []\n",
                "    for c, (X_test, y_test) in zip(clfs, datasets):\n",
                "        scores.append(c.score(X_test, y_test))\n",
                "\n",
                "    return scores\n",
                "\n",
                "\n",
                "def hexToRGBA(hex, alpha):\n",
                "\n",
                "    \"\"\"\n",
                "    Function that returns an rgba value from an hex and an opacity value\n",
                "    \n",
                "    :param (String) clf: Hex value \n",
                "    :param (float) params: Value between 0 and 1 indicating opacity\n",
                "\n",
                "    \"\"\"  \n",
                "\n",
                "    r = int(hex[1:3], 16)\n",
                "    g = int(hex[3:5], 16)\n",
                "    b = int(hex[5:], 16)\n",
                "\n",
                "    if alpha:\n",
                "        return \"rgba(\" + str(r) + \", \" + str(g) + \", \" + str(b) + \", \" + str(alpha) + \")\"\n",
                "    else:\n",
                "        return \"rgb(\" + str(r) + \", \" + str(g) + \", \" + str(b) + \")\"\n",
                "\n",
                "\n",
                "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=-1, train_sizes=np.linspace(.008, 1.0, 5)):\n",
                "    \"\"\"\n",
                "    Generate a simple plot of the test and training learning curve.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
                "        An object of that type which is cloned for each validation.\n",
                "\n",
                "    title : string\n",
                "        Title for the chart.\n",
                "\n",
                "    X : array-like, shape (n_samples, n_features)\n",
                "        Training vector, where n_samples is the number of samples and\n",
                "        n_features is the number of features.\n",
                "\n",
                "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
                "        Target relative to X for classification or regression;\n",
                "        None for unsupervised learning.\n",
                "\n",
                "    cv : int, cross-validation generator or an iterable, optional\n",
                "        Determines the cross-validation splitting strategy.\n",
                "        Possible inputs for cv are:\n",
                "          - None, to use the default 3-fold cross-validation,\n",
                "          - integer, to specify the number of folds.\n",
                "          - :term:`CV splitter`,\n",
                "          - An iterable yielding (train, test) splits as arrays of indices.\n",
                "\n",
                "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
                "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
                "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
                "\n",
                "        Refer :ref:`User Guide <cross_validation>` for the various\n",
                "        cross-validators that can be used here.\n",
                "\n",
                "    n_jobs : int or None, optional (default=None)\n",
                "        Number of jobs to run in parallel.\n",
                "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
                "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
                "        for more details.\n",
                "\n",
                "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
                "        Relative or absolute numbers of training examples that will be used to\n",
                "        generate the learning curve. If the dtype is float, it is regarded as a\n",
                "        fraction of the maximum size of the training set (that is determined\n",
                "        by the selected validation method), i.e. it has to be within (0, 1].\n",
                "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
                "        Note that for classification the number of samples usually have to\n",
                "        be big enough to contain at least one sample from each class.\n",
                "        (default: np.linspace(0.1, 1.0, 5))\n",
                "    \"\"\"\n",
                "    \n",
                "    train_sizes, train_scores, test_scores = learning_curve(estimator, \n",
                "                                                            X, \n",
                "                                                            y, \n",
                "                                                            cv=cv, \n",
                "                                                            n_jobs=n_jobs, \n",
                "                                                            train_sizes=train_sizes, \n",
                "                                                            scoring=\"f1\", \n",
                "                                                            random_state=RANDOM_SEED)\n",
                "    train_scores_mean = np.mean(train_scores, axis=1)\n",
                "    train_scores_std = np.std(train_scores, axis=1)\n",
                "    test_scores_mean = np.mean(test_scores, axis=1)\n",
                "    test_scores_std = np.std(test_scores, axis=1)\n",
                "    \n",
                "    # Prints lower bound (mean - std) of train \n",
                "    trace1 = go.Scatter(\n",
                "        x=train_sizes, \n",
                "        y=train_scores_mean - train_scores_std, \n",
                "        showlegend=False,\n",
                "        mode=\"lines\",\n",
                "        name=\"\",\n",
                "        hoverlabel = dict(\n",
                "            namelength=20\n",
                "        ),\n",
                "        line = dict(\n",
                "            width = 0.1,\n",
                "            color = hexToRGBA(PLOTLY_COLORS[0], 0.4),\n",
                "        ),\n",
                "    )\n",
                "    # Prints upper bound (mean + std) of train\n",
                "    trace2 = go.Scatter(\n",
                "        x=train_sizes, \n",
                "        y=train_scores_mean + train_scores_std, \n",
                "        showlegend=False,\n",
                "        fill=\"tonexty\",\n",
                "        mode=\"lines\",\n",
                "        name=\"\",\n",
                "        hoverlabel = dict(\n",
                "            namelength=20\n",
                "        ),\n",
                "        line = dict(\n",
                "            width = 0.1,\n",
                "            color = hexToRGBA(PLOTLY_COLORS[0], 0.4),\n",
                "        ),\n",
                "    )\n",
                "    \n",
                "    # Prints mean train score line\n",
                "    trace3 = go.Scatter(\n",
                "        x=train_sizes, \n",
                "        y=train_scores_mean, \n",
                "        showlegend=True,\n",
                "        name=\"Train score\",\n",
                "        line = dict(\n",
                "            color = PLOTLY_COLORS[0],\n",
                "        ),\n",
                "    )\n",
                "    \n",
                "    # Prints lower bound (mean - std) of test \n",
                "    trace4 = go.Scatter(\n",
                "        x=train_sizes, \n",
                "        y=test_scores_mean - test_scores_std, \n",
                "        showlegend=False,\n",
                "        mode=\"lines\",\n",
                "        name=\"\",\n",
                "        hoverlabel = dict(\n",
                "            namelength=20\n",
                "        ),\n",
                "        line = dict(\n",
                "            width = 0.1,\n",
                "            color = hexToRGBA(PLOTLY_COLORS[1], 0.4),\n",
                "        ),\n",
                "    )\n",
                "        # Prints upper bound (mean + std) of test\n",
                "    trace5 = go.Scatter(\n",
                "        x=train_sizes, \n",
                "        y=test_scores_mean + test_scores_std, \n",
                "        showlegend=False,\n",
                "        fill=\"tonexty\",\n",
                "        mode=\"lines\",\n",
                "        name=\"\",\n",
                "        hoverlabel = dict(\n",
                "            namelength=20\n",
                "        ),\n",
                "        line = dict(\n",
                "            width = 0.1,\n",
                "            color = hexToRGBA(PLOTLY_COLORS[1], 0.4),\n",
                "        ),\n",
                "    )\n",
                "\n",
                "    # Prints mean test score line \n",
                "    trace6 = go.Scatter(\n",
                "        x=train_sizes, \n",
                "        y=test_scores_mean, \n",
                "        showlegend=True,\n",
                "        name=\"Test score\",\n",
                "        line = dict(\n",
                "            color = PLOTLY_COLORS[1],\n",
                "        ),\n",
                "    )\n",
                "    \n",
                "    data = [trace1, trace2, trace3, trace4, trace5, trace6]\n",
                "    layout = go.Layout(\n",
                "        title=title,\n",
                "        autosize=True,\n",
                "        yaxis=dict(\n",
                "            title='F1 Score',\n",
                "        ),\n",
                "        xaxis=dict(\n",
                "            title=\"#Training samples\",\n",
                "        ),\n",
                "        legend=dict(\n",
                "            x=0.8,\n",
                "            y=0,\n",
                "        ),\n",
                "    )\n",
                "    fig = go.Figure(data=data, layout=layout)\n",
                "    return iplot(fig, filename=title)\n",
                "\n",
                "\n",
                "def print_confusion_matrix(gs, X_test, y_test):\n",
                "\n",
                "    \"\"\"\n",
                "    Function that prints confusion matrix for a classifier\n",
                "    \n",
                "    :param (estimator) clf: Classifier object\n",
                "    :param (array-like) X_test: List of data to be tested with\n",
                "    :param (array-like) y_test: List of labels for test \n",
                "    \"\"\"  \n",
                "\n",
                "    gs_score = gs.score(X_test, y_test)\n",
                "    y_pred = gs.predict(X_test)\n",
                "\n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    t = PrettyTable()\n",
                "    t.add_row([\"True Edible\", cm[0][0], cm[0][1]])\n",
                "    t.add_row([\"True Poisonous\", cm[1][0], cm[1][1]])\n",
                "    t.field_names = [\" \", \"Predicted Edible\", \"Predicted Poisonous\"]\n",
                "    print(t)\n",
                "\n",
                "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix\n",
                "    cm_df = pd.DataFrame(cm.round(3), index=[\"True edible\", \"True Poisonous\"], columns=[\"Predicted edible\", \"Predicted poisonous\"])\n",
                "    cm_df\n",
                "\n",
                "\n",
                "def print_raw_score(clf, X_test, y_test):\n",
                "    \"\"\"\n",
                "    Function that scores a classifier on some data\n",
                "    \n",
                "    :param (array of estimator) clf: Array of classifiers\n",
                "    :param (array-like) X_test: List of data to be tested with\n",
                "    :param (array-like) y_test: List of labels for test \n",
                "\n",
                "    \"\"\"  \n",
                "    print(\"Score achieved by NB: %0.3f\" % (score([clf], [(X_test, y_test)])[0]))\n",
                "\n",
                "\n",
                "def plot_feature_importance(feature_importance, title):\n",
                "    \"\"\"\n",
                "    Function that plots feature importance for a decision tree or a random forest classifier\n",
                "    \n",
                "    :param (dictionary) feature_importance: Dictionary of most important features sorted\n",
                "    :param (str) title: Title of the plot\n",
                "\n",
                "    \"\"\" \n",
                "    \n",
                "    trace1 = go.Bar(\n",
                "        x=feature_importance[:, 0],\n",
                "        y=feature_importance[:, 1],\n",
                "        marker = dict(color = PLOTLY_COLORS[0]),\n",
                "        opacity=PLOTLY_OPACITY,\n",
                "        name='Feature importance'\n",
                "    )\n",
                "    data = [trace1]\n",
                "    layout = go.Layout(\n",
                "        title=title,\n",
                "        autosize=True,\n",
                "        margin=go.layout.Margin(l=50, r=100, b=150),\n",
                "        xaxis=dict(\n",
                "            title='feature',\n",
                "            tickangle=30\n",
                "        ),\n",
                "        yaxis=dict(\n",
                "            title='feature importance',\n",
                "            automargin=True,\n",
                "        ),\n",
                "    )\n",
                "    fig = go.Figure(data=data, layout=layout)\n",
                "    return iplot(fig, filename=title)\n",
                "\n",
                "\n",
                "def print_performances(classifiers, classifier_names, auc_scores, X_test, y_test):\n",
                "  \n",
                "    \"\"\"\n",
                "    Function that scores a classifier on some data\n",
                "    \n",
                "    :param (array of estimator) clf: Array of classifiers\n",
                "    :param (array-like) classifier_names: Title of the classifier\n",
                "    :param (array-like) auc-score: Auc scores\n",
                "    :param (array-like) X_test: List of data to be tested with\n",
                "    :param (array-like) y_test: List of labels for test \n",
                "\n",
                "    \"\"\" \n",
                "\n",
                "    accs = []\n",
                "    recalls = []\n",
                "    precision = []\n",
                "    results_table = pd.DataFrame(columns=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"auc\"])\n",
                "    for (i, clf), name, auc in zip(enumerate(classifiers), classifier_names, auc_scores):\n",
                "        y_pred = clf.predict(X_test)\n",
                "        row = []\n",
                "        row.append(accuracy_score(y_test, y_pred))\n",
                "        row.append(precision_score(y_test, y_pred))\n",
                "        row.append(recall_score(y_test, y_pred))\n",
                "        row.append(f1_score(y_test, y_pred))\n",
                "        row.append(auc)\n",
                "        row = [\"%.3f\" % r for r in row]\n",
                "        results_table.loc[name] = row\n",
                "    return results_table\n",
                "\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " Let's start defining the Stratified K-Folds cross-validator; it provides train/test indices to split data in train/test sets.\n",
                "\n",
                " This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of\n",
                " samples for each class. Stratification is generally a better scheme, both in terms of bias and variance, when compared to\n",
                " regular cross-validation.\n",
                "\n",
                " Then we define our classifiers, setting the `random_state` parameter to our usual seed value,\n",
                " in such a way that we can classify each time in the same way."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "kf = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED)\n",
                "clf_nb = GaussianNB()\n",
                "clf_knn = KNeighborsClassifier()\n",
                "clf_rf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
                "clf_lr = LogisticRegression(random_state=RANDOM_SEED)\n",
                "clf_svm = SVC(random_state=RANDOM_SEED)\n",
                "clfs = [clf_nb, clf_knn, clf_rf, clf_lr, clf_svm]\n",
                "TEST_PARAMS = {}\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " We set `TEST_PARAMS` empty, because in this phase we are not interested in tuning parameters."
            ],
            "metadata": {}
        },
        {
            "source": [
                "all_test_results = []\n",
                "all_gss = []\n",
                "times = np.zeros(7)\n",
                "t = 0\n",
                "      \n",
                "for clf in clfs:\n",
                "\n",
                "  gs_full, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train, y_train, kf, execution_time=True)\n",
                "  times[0] = times[0] + float(t)\n",
                "  \n",
                "  gs_pc, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_pc, y_train_pc, kf, execution_time=True)\n",
                "  times[1] = times[1] + float(t)\n",
                "  \n",
                "  gs_drop, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_drop, y_train_drop, kf, execution_time=True)\n",
                "  times[2] = times[2] + float(t)\n",
                "  \n",
                "  gs_pc_drop, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_pc_drop, y_train_pc_drop, kf, execution_time=True)\n",
                "  times[3] = times[3] + float(t)\n",
                "  \n",
                "  gs_no_stalk, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_no_stalk, y_train_no_stalk, kf, execution_time=True)\n",
                "  times[4] = times[4] + float(t)\n",
                "  \n",
                "  gs_ohc_pc, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_ohc_pc, y_train_ohc_pc, kf, execution_time=True)\n",
                "  times[5] = times[5] + float(t)\n",
                "  \n",
                "  gs_ohc, t = param_tune_grid_cv(clf, TEST_PARAMS, X_train_ohc, y_train_ohc, kf, execution_time=True)\n",
                "  times[6] = times[6] + float(t)\n",
                "  \n",
                "  \n",
                "  gss = [gs_full, gs_pc, gs_drop, gs_pc_drop, gs_no_stalk, gs_ohc_pc, gs_ohc]\n",
                "  all_gss.append(gss)\n",
                "  test_results = score(gss, [(X_test, y_test), \n",
                "                             (X_test_pc, y_test_pc), \n",
                "                             (X_test_drop, y_test_drop), \n",
                "                             (X_test_pc_drop, y_test_pc_drop),\n",
                "                             (X_test_no_stalk, y_test_no_stalk),\n",
                "                             (X_test_ohc_pc, y_test_ohc_pc),\n",
                "                             (X_test_ohc, y_test_ohc)])\n",
                "  all_test_results.append(test_results)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " This is the score of the different classifiers on the different datasets."
            ],
            "metadata": {}
        },
        {
            "source": [
                "dataset_strings = [\" \", \"full dataset\", \n",
                "                   \"dataset reduced on first 9 PC\", \n",
                "                   \"dataset with dropped missing values\",\n",
                "                   \"dataset with dropped missing value reduced with first 9 PC\",\n",
                "                   \"dataset with stalk-root field dropped\",\n",
                "                   \"dataset ohc reduced on first 20 PC\",\n",
                "                   \"dataset ohc\"]\n",
                "\n",
                "row_names = [\"Naive Bayes\", \"KNN\", \"Random Forest\", \"Logistic Regression\", \"SVM\"]\n",
                "\n",
                "t = PrettyTable()\n",
                "t.field_names = dataset_strings\n",
                "\n",
                "all_rows = []\n",
                "result_row = []\n",
                "\n",
                "for name, results in zip(row_names, all_test_results):\n",
                "  result_row.append(name)\n",
                "  for r in results:\n",
                "    result_row.append(\"%.3f\" % r)\n",
                "  all_rows.append(result_row)\n",
                "  result_row = []\n",
                " \n",
                "all_rows = sorted(all_rows, key=lambda kv: kv[1], reverse=True)\n",
                "\n",
                "for k in all_rows:\n",
                "    t.add_row(k)\n",
                "    \n",
                "print(t)"
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " Looking at the table we can notice that the Naive Bayes classifier performed very poorly on the dataset where we dropped\n", 
                " the rows containing the missing values. If we look deeper in its confusion matrix, we can notice that\n",
                " almost all edible mushrooms are classified correcly, while almost all of the poisonous ones are misclassified. This\n",
                " happens because the probability of finding an edible sample is much higher than the one of finding a\n",
                " poisonous one, and the classifies picks almost always the edible class due to this reason. To address\n",
                " this problem, we should apply some over/under sampling methods (or a combination of them), such as\n",
                " SMOTE or RandomOver(Under)Sampler."
            ],
            "metadata": {}
        },
        {
            "source": [
                "print_confusion_matrix(all_gss[0][np.argmin(all_test_results[0])], X_test_drop, y_test_drop)"
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " After having evaluated all these results, we are going to select the one that\n",
                " have the best tradeoff between time and score. Let's calculate the mean score and the overall time taken to train all\n",
                " the different classifiers with respect to the same dataset."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "means = np.mean(all_test_results, axis=0)\n",
                "  \n",
                "time_table = PrettyTable()\n",
                "time_row = [\"Total train time (s)\"] + np.ndarray.tolist(times)\n",
                "mean_row = [\"Mean score for dataset\"] + np.ndarray.tolist(means)\n",
                "time_table.field_names = dataset_strings\n",
                "time_table.add_row(time_row)\n",
                "time_table.add_row(mean_row)\n",
                "print(time_table)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "  \n",
                "print(\"The dataset that gives the best overall performances is:\")\n",
                "print(\"\\t- \" + dataset_strings[means.argmax()] + \", with a score of \" + str(\"%.3f\" % means.max()))\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " As we can see, the most accurate is the dataset encoded with `OneHotEncoder`; The training time though, is really high\n",
                " with respect to the other datasets. A good trade-off could be using the full dataset, because the accuracy is high and the\n",
                " training time is reasonable.\n",
                "\n",
                " But considering that the scores obtained are achieved without parameter tuning, I prefer to choose a \"faster\" dataset, even if the scores\n",
                " are not already optimal.\n",
                " From this point on, the classifiers will be tested on the full dataset with PCA. This because the cross-validation phase\n",
                " is almost four times faster, and the scores are similar."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### Logistic Regression\n",
                " The first classifier that we will analyze is the Logistic Regression classifier. It uses the\n",
                " sigmoid function to classify our samples:\n",
                "\n",
                " - $P(y=0 | X;\\theta) = g(w^T X) = \\frac{1}{1+e^{w^T X}}$\n",
                " - $P(y=1 | X;\\theta) = 1 - g(w^T X) = \\frac{e^{w^T X}}{1+e^{w^T X}}$\n",
                "\n",
                "\n",
                " This model, with respect to linear regression, can model better the zone close to\n",
                " 0 and 1. To learn the weights, the $MLE$ is found and then the gradient descent algorithm\n",
                " is applied until the accuracy converges.\n",
                "\n",
                " They are:\n",
                " - `liblinear`. Solver, which is better for smaller datasets\n",
                " - `C`. Regularization strength, ranging from 0.01 to 100. A smaller value inidicates stronger regularization, like in svms.\n",
                " - `penalty`. \"l1\" and \"l2\" penalty for regularization, which are defined as:\n",
                "   - l1, it penalizes every mistake at the same way\n",
                "       - $S = \\Sigma_{i=1}^{n}{|y_i - f(x_i)|}$\n",
                "\n",
                "   - l2, it penalizes bigger values\n",
                "       - $S = \\Sigma_{i=1}^{n}{(y_i - f(x_i))^2}$\n",
                "\n",
                "   - Where $y_i$ is the true label and $f(x_i)$ is the assigned label."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "clf_lr = LogisticRegression(random_state=RANDOM_SEED)\n",
                "gs_pc_lr = param_tune_grid_cv(clf_lr, LOGISTIC_REGRESSION_PARAMS, X_train_pc, y_train_pc, kf)\n",
                "print_gridcv_scores(gs_pc_lr, n=5)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "plot_learning_curve(gs_pc_lr.best_estimator_, \"Learning curve of Logistic Regression\", \n",
                "                    X_train_pc,\n",
                "                    y_train_pc,\n",
                "                    cv=5)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " From the graph we can see that the two lines approach themselves ans the test score\n",
                " tends to the train score. In this case, from the graph is evident that adding more\n",
                " samples is useless; we have more than enough, but the model is not complex enough to\n",
                " anchieve higher scores."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " From the learning curve we can see that, at the beginning"
            ],
            "metadata": {}
        },
        {
            "source": [
                "print_confusion_matrix(gs_pc_lr, X_test_pc, y_test_pc)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### Support vector machine\n",
                " A Support Vector Machine (SVM) is a discriminative classifier formally defined by a\n",
                " separating hyperplane. In other words, given labeled training data (supervised learning),\n",
                " the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space\n",
                " this hyperplane is a line dividing a plane in two parts where in each class lay in either side.\n",
                "\n",
                " We perform the grid search over:\n",
                " - `linear`. This is the simplest SVM, finds the hyperplane which separates in the best way our samples.\n",
                " - `C`. The C parameter tells the SVM optimization how much you want to avoid misclassifying\n",
                " each training example. For large values of C, the optimization will choose a smaller-margin\n",
                " hyperplane if that hyperplane does a better job of getting all the training points\n",
                " classified correctly.\n",
                " - `rbf`. This parameter indicates that we are using a radial basis function kernel to perform the\n",
                " scalar product.\n",
                "   - `gamma`. Defines how far the influence of a single training example reaches,\n",
                " with low values meaning ‘far’ and high values meaning ‘close’."
            ],
            "metadata": {}
        },
        {
            "source": [
                "clf_svm = SVC(probability=True, random_state=RANDOM_SEED)\n",
                "gs_pc_svm = param_tune_grid_cv(clf_svm, SVM_PARAMS, X_train_pc, y_train_pc, kf)\n",
                "print_gridcv_scores(gs_pc_svm, n=5)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "plot_learning_curve(gs_pc_svm.best_estimator_, \"Learning curve of SVM\", \n",
                "                    X_train_pc,\n",
                "                    y_train_pc,\n",
                "                    cv=5)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " We can see from the learning curve that we can achieve optimal performances\n",
                " before running out of training samples. After approximately 1300 samples analyzed,\n",
                " we already achieve a test score greater than 0.99 with very low standard deviation.\n",
                "\n",
                " This is the best score possibly achievable.\n",
                " We could expect it, but we will print anyway the confusion matrix:\n",
                ""
            ],
            "metadata": {}
        },
        {
            "source": [
                "print_confusion_matrix(gs_pc_svm, X_test_pc, y_test_pc)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### Naive Bayes Classifier\n",
                " The naive bayes classifier is based on the Bayes theorem, which states that:\n",
                "\n",
                " - $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
                "\n",
                " Where:\n",
                " - $P(A)$, the prior, is the initial degree of belief in A.\n",
                " - $P(A|B)$, the posterior is the degree of belief having accounted for B.\n",
                " - $P(B|A$), is the likelyhood, the degree of belief in B, given that A is true.\n",
                "\n",
                "\n",
                " Using Bayes theorem, we can find the probability of A happening,\n",
                " given that B has occurred. Here, B is the evidence and A is the hypothesis. The assumption made\n",
                " here is that the predictors/features are independent. That is presence of one particular\n",
                " feature does not affect the other. Hence it is called naive."
            ],
            "metadata": {}
        },
        {
            "source": [
                "clf_nb = GaussianNB()\n",
                "gs_pc_nb = param_tune_grid_cv(clf_nb, TEST_PARAMS, X_train_pc, y_train_pc, kf)\n",
                "print_gridcv_scores(gs_pc_nb, n=5)\n",
                "\n",
                "print_confusion_matrix(gs_pc_nb, X_test_pc, y_test_pc)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "plot_learning_curve(clf_nb, \"Learning curve of GaussianNB\", \n",
                "                    X_train, \n",
                "                    y_train, \n",
                "                    cv=5)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " To be a really simple classifier, it can achieve decent results. The independecy assumption\n",
                " does not work perfectly, because the different features are never completely independent from\n",
                " each other, but nevertheless it manages to achieve a good accuracy even if we used only\n",
                " the data projected on the principal components.\n",
                "\n",
                " With the naive bayes classifier, it is always\n",
                " better to use the original dataset, being really fast itself to train and to classify samples.\n",
                " With the ohc dataset, if we look in the upper table, we achieve a score around 0.945, which is\n",
                " a great score given our big assumption."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### Random Forest Classifier\n",
                " It is a tree-based method, derived from bagging decision trees, to which\n",
                " it is added another small trick which decorrelates the trees, in order to\n",
                " reduce further the variance.\n",
                "\n",
                " As in bagging, we build a number of decision trees on bootstrapped training\n",
                " samples. But when building these decision trees, each time a split in a tree\n",
                " is considered, a random selection of m predictors is chosen as split\n",
                " candidates from the full set of p predictors. The split is allowed to use only\n",
                " one of those m predictors, which typically are $m \\approx \\sqrt{p}$."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "clf_pc_rf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
                "gs_pc_rf = param_tune_grid_cv(clf_pc_rf, RANDOM_FOREST_PARAMS, X_train_pc, y_train_pc, kf)\n",
                "print_gridcv_scores(gs_pc_rf, n = 5)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "print_confusion_matrix(gs_pc_rf, X_test_pc, y_test_pc)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "plot_learning_curve(gs_pc_rf.best_estimator_, \"Learning curve of Random Forest Classifier\", \n",
                "                    X_train_pc,\n",
                "                    y_train_pc,\n",
                "                    cv=5)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " The training time is pretty high, but the accuracy as well. Looking at the learning curve, we can notice that\n",
                " the test error approaches the training error, but at a certain point starts to decrease. Analyzing over 1000 samples the\n",
                " accuracy does not improve any further. This may be due to some overfitting effect. A method to improve the accuracy could\n",
                " be using some additional regularization techniques, using for example `LightGMB` library to better fine-tune\n",
                " additional parameter.\n",
                "\n",
                " Now let's look deeper into the features of the Random Forest Classifier; let's see which of them weights more\n",
                " on the classification."
            ],
            "metadata": {}
        },
        {
            "source": [
                "feature_importance = np.array(  sorted(zip(X_train_pc.columns, \n",
                "                                gs_pc_rf.best_estimator_.named_steps['clf'].feature_importances_),\n",
                "                                key=lambda x: x[1], reverse=True))\n",
                "plot_feature_importance(feature_importance, \"Feature importance in the random forest\")\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " Here we just see which are the components that weight more in the classification, but due to the\n",
                " fact that we reduced the dataset by means of PCA, we only see which component weighted more in\n",
                " the node splitting phase. Why the last components weight more than the first, the ones\n",
                " which contain most of the variability?\n",
                "\n",
                " The first principal component is a linear combination of all our features. The fact that it explains almost all the\n",
                " variability just means that most of the coefficients of the variables in the first principal component are significant.\n",
                " The classification trees we generate do binary splits on\n",
                " continuous variables that best separate the categories we want to classify. That is not exactly\n",
                " the same as finding orthogonal linear combinations of continuous variables that give the direction\n",
                " of greatest variance.\n",
                "\n",
                " If we would use the full dataset, we would see that the most important features are:\n",
                "\n",
                " 1. `bruises`\n",
                " 2. `gill-size`\n",
                " 3. `gill-spacing`"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### K-Nearest Neighbors Classifier\n",
                " K-NN is a type of instance-based learning, or lazy learning, where the function\n",
                " is only approximated locally and all computation is deferred until classification.\n",
                " The K-NN algorithm is among the simplest of all machine learning algorithms.\n",
                "\n",
                " The training phase of the algorithm consists only of storing the feature vectors\n",
                " and class labels of the training samples. In the classification phase, K is a user-defined constant,\n",
                " and an unlabeled vector (a query or test point) is classified by assigning the label which\n",
                " is most frequent among the k training samples nearest to that query point.\n",
                "\n",
                " The parameters for cross validation are:\n",
                " - `n_neighbors`. Number of closes samples to analyze\n",
                " - `weights`. Indicates the weight function to use in prediction.\n",
                "   - `uniform`. All points in the neighborhood are weighted equally.\n",
                "   - `distance`. Weight points by the inverse of their distance.\n",
                " In this case, closer neighbors of a query point will have a greater\n",
                " influence than neighbors which are further away.\n",
                " - `p`. Power parameter for the Minkowski metric (generalization of Euclidean distance)\n",
                "   - p=1 uses `l1`\n",
                "   - p=2 uses `l2`\n",
                "   - p>2 minkowski_distance (l_p) is used."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "clf_knn = KNeighborsClassifier()\n",
                "gs_knn = param_tune_grid_cv(clf_knn, KNN_PARAMS, X_train_pc, y_train_pc, kf)\n",
                "print_gridcv_scores(gs_knn, n=5)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "print_confusion_matrix(gs_knn, X_train_pc, y_train_pc)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "\n",
                "plot_learning_curve(gs_knn.best_estimator_, \"Learning curve of k-NN Classifier\", \n",
                "                    X_train_pc,\n",
                "                    y_train_pc,\n",
                "                    cv=5)\n",
                "\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " The accuracy achieved is really high.\n",
                " The K-NN classification with the whole dataset gives the same result but it takes more than\n",
                " 7 times more time"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " ### ROC curve\n",
                " At this point we need to evaluate the performances of the different classifiers and\n",
                " compare them.\n",
                " We are going to plot the ROC curve and the Area Under Curve for all our models.\n",
                " classifiers.\n",
                " The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.\n",
                " Specifically, these parameters are:\n",
                "\n",
                " - $TRP/Recall/Sensitivity = \\frac{TP}{TP+FN}$\n",
                " - $FPR = \\frac{FP}{TN+FP}$\n",
                "\n",
                "\n",
                " An excellent model has AUC near to the 1 which means it has good measure of\n",
                " separability. A poor model has AUC near to the 0 which means it has worst measure\n",
                " of separability."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "def plot_roc_curve(classifiers, legend, title, X_test, y_test):\n",
                "    t1 = go.Scatter(\n",
                "        x=[0, 1], \n",
                "        y=[0, 1], \n",
                "        showlegend=False,\n",
                "        mode=\"lines\",\n",
                "        name=\"\",\n",
                "        line = dict(\n",
                "            color = COLOR_PALETTE[0],\n",
                "        ),\n",
                "    )\n",
                "    \n",
                "    data = [t1]\n",
                "    aucs = []\n",
                "    for clf, string, c in zip(classifiers, legend, COLOR_PALETTE[1:]):\n",
                "        y_test_roc = np.array([([0, 1] if y else [1, 0]) for y in y_test])\n",
                "        y_score = clf.predict_proba(X_test)\n",
                "        \n",
                "        # Compute ROC curve and ROC area for each class\n",
                "        fpr = dict()\n",
                "        tpr = dict()\n",
                "        roc_auc = dict()\n",
                "        for i in range(2):\n",
                "            fpr[i], tpr[i], _ = roc_curve(y_test_roc[:, i], y_score[:, i])\n",
                "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
                "\n",
                "        # Compute micro-average ROC curve and ROC area\n",
                "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_roc.ravel(), y_score.ravel())\n",
                "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
                "        aucs.append(roc_auc['micro'])\n",
                "\n",
                "        trace = go.Scatter(\n",
                "            x=fpr['micro'], \n",
                "            y=tpr['micro'], \n",
                "            showlegend=True,\n",
                "            mode=\"lines\",\n",
                "            name=string + \" (area = %0.2f)\" % roc_auc['micro'],\n",
                "            hoverlabel = dict(\n",
                "                namelength=30\n",
                "            ),\n",
                "            line = dict(\n",
                "                color = c,\n",
                "            ),\n",
                "        )\n",
                "        data.append(trace)\n",
                "\n",
                "    layout = go.Layout(\n",
                "        title=title,\n",
                "        autosize=False,\n",
                "        width=550,\n",
                "        height=550,\n",
                "        yaxis=dict(\n",
                "            title='True Positive Rate',\n",
                "        ),\n",
                "        xaxis=dict(\n",
                "            title=\"False Positive Rate\",\n",
                "        ),\n",
                "        legend=dict(\n",
                "            x=0.4,\n",
                "            y=0.06,\n",
                "        ),\n",
                "    )\n",
                "    fig = go.Figure(data=data, layout=layout)\n",
                "    return aucs, iplot(fig, filename=title)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "\n",
                "classifiers = [gs_pc_lr, gs_pc_svm, gs_pc_nb, gs_pc_rf, gs_knn]\n",
                "classifier_names = [\"Logistic Regression\", \"SVM\", \"GaussianNB\", \"Random Forest\", \"KNN\"]\n",
                "auc_scores, roc_plot = plot_roc_curve(classifiers, classifier_names, \"ROC curve\", X_test_pc, y_test_pc)\n",
                "roc_plot\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " Now let's look at some additional parameters that evaluate the goodness of our models:\n",
                "\n",
                " 1. Accuracy\n",
                " 2. Precision\n",
                "       - $P = \\frac{TP}{TP+FP}$\n",
                " 3. Recall\n",
                "       - $R = \\frac{TP}{TP+FN}$\n",
                " 4. F1 (weighted average of the precision and recall)\n",
                "       - $F1= 2*\\frac{P * R}{P + R}$\n",
                " 5. Auc, area under the ROC curve"
            ],
            "metadata": {}
        },
        {
            "source": [
                "print_performances(classifiers, classifier_names, auc_scores, X_test_pc, y_test_pc)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " The table shows that overall all classifiers performed well. The naive Bayes classifier\n",
                " is the one that performed worse, being the simplest one of them. Another reason is because\n",
                " we used the reduced dataset in classification; due to the fact that it is really fast,\n",
                " with the naive bayes is better to use the whole dataset. Infact\n",
                " the one OneHotEncoded anchieves a really high score, namely 0,945 (see score table in the choice\n",
                " of the dataset phase).\n",
                " The SVM and the K-NN are the classifiers that achieved the best score, hitting the 1 value on every parameter.\n",
                " The the Random Forest classifier achieved a similar score, even if slightly\n",
                " worse in accuracy and in recall."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                " ## What NOT to do in the woods\n",
                "\n",
                " To have a little bit more fun we will try one more thing.\n",
                " If we are in a wood, how can we survive without the help of an SVM?\n",
                " Let's find out what are the peculiar traits of a poisonous mushroom\n",
                "\n",
                " Firstly, we create a KNN classifier and we iterate on all the columns,\n",
                " to see which of them gives a more accurate classification;\n",
                " In this way we can see which ones are the most important charateristics to\n",
                " classify a mushroom."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "n_features = pre_ohc_data.shape[1]\n",
                "clf = KNeighborsClassifier()\n",
                "feature_score = []\n",
                "t = PrettyTable()\n",
                "t.field_names = [\"Feature\", \"Score\"]\n",
                "\n",
                "for i in range(n_features):\n",
                "    X_feature= np.reshape(pre_ohc_data.iloc[:,i:i+1],-1,1) # One column at a time\n",
                "    scores = cross_val_score(clf, X_feature, y_data)\n",
                "    feature_score.append(scores.mean())\n",
                "    t.add_row([pre_ohc_data.columns[i], \"{0:0.4f}\".format(scores.mean())])\n",
                "\n",
                "print(t)\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " Let's now select all the features that are more significant; we will\n",
                " pick the ones with a score greater than 0.7"
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "f_importance = pd.Series(data = feature_score, index = pre_ohc_data.columns)\n",
                "f_importance.sort_values(ascending=False, inplace=True)\n",
                "f_importance[f_importance > 0.7]\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " Now we merge the unlabelled dataset with the labels, in such a way that\n",
                " we can group our samples using the class."
            ],
            "metadata": {}
        },
        {
            "source": [
                "\n",
                "col_importance = f_importance[f_importance>0.7].index.values\n",
                "pre_ohc_Xy = pd.concat([pre_ohc_data, pd.DataFrame(y_data, columns=['class'])], axis=1)\n",
                "grouped = pre_ohc_Xy.groupby('class')\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "feat_edible = grouped.get_group(0)[col_importance].sum()\n",
                "feat_edible"
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "source": [
                "feat_poisonous = grouped.get_group(1)[col_importance].sum()\n",
                "feat_poisonous\n",
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                " <a class=\"anchor-link\" href=\"#conclusions\">¶</a>\n",
                " ## Conclusions\n",
                " Our goal was to predict if a mushroom was poisonous or edible from its features.\n",
                "\n",
                " We understood that they are well separated and our classifiers can anchieve optimal performances.\n",
                "\n",
                "\n",
                " ![](https://infovisual.info/storage/app/media/01/img_en/024%20Mushroom.jpg)\n",
                "\n",
                "\n",
                " Looking at the KNN score using the different values for features, we understood that you should\n",
                " not eat a mushroom if it has:\n",
                "\n",
                " 1. fishy odor\n",
                " 2. stalk surface above ring silky\n",
                " 3. stalk surface below ring silky\n",
                " 4. gill size narrow\n",
                " 5. spore print color chocolatey"
            ],
            "metadata": {}
        },
        {
            "source": [
                ""
            ],
            "cell_type": "code",
            "outputs": [],
            "metadata": {},
            "execution_count": 0
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "language_info": {
            "name": "python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            }
        },
        "orig_nbformat": 2,
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "python",
        "npconvert_exporter": "python",
        "pygments_lexer": "ipython3",
        "version": 3
    }
}
