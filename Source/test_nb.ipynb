{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\nimport os\ntry:\n\tos.chdir(os.path.join(os.getcwd(), '..'))\n\tprint(os.getcwd())\nexcept:\n\tpass\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Safe to eat or deadly poisonous?\n","## An analysis on mushroom classification by Lorenzo Santolini"],"metadata":{}},{"cell_type":"markdown","source":["### Code snippet for google colab"],"metadata":{}},{"source":["# Little code snippet to import on Google Colab the dataset\n","'''\n","!pip install -U -q kaggle\n","!mkdir -p ~/.kaggle\n","\n","# Insert here your kaggle API key\n","from google.colab import files\n","files.upload()\n","\n","!cp kaggle.json ~/.kaggle/\n","!kaggle datasets download -d uciml/mushroom-classification\n","!unzip mushroom-classification.zip\n","!ls\n","'''\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Constants\n","PLOTLY_COLORS = ['#140DFF', '#FF0DE2']\n","COLOR_PALETTE = ['#FF0DE2', '#BD0CE8', '#8B00FF', '#4D0FE8', '#140DFF']\n","COLORSCALE_HEATMAP = [         [0.0, 'rgb(70,0,252)'], \n","                [0.1111111111111111, 'rgb(78,0,252)'], \n","                [0.2222222222222222, 'rgb(90,0,252)'], \n","                [0.3333333333333333, 'rgb(110,0,248)'], \n","                [0.4444444444444444, 'rgb(130,0,238)'], \n","                [0.5555555555555556, 'rgb(145,0,228)'], \n","                [0.6666666666666666, 'rgb(166,0,218)'], \n","                [0.7777777777777778, 'rgb(187,0,213)'], \n","                [0.8888888888888888, 'rgb(200,0,202)'], \n","                               [1.0, 'rgb(210,0,191)']]\n","PLOTLY_OPACITY = 0.7\n","RANDOM_SEED = 11\n","\n","LOGISTIC_REGRESSION_PARAMS = {\n","    'clf__solver': ['liblinear'],  # best for small datasets\n","    'clf__C': [0.01, 0.1, 1, 10, 100], # smaller value, stronger regularization\n","    'clf__penalty': ['l2', 'l1']\n","}\n","\n","SVM_PARAMS = [\n","{\n","    'clf__kernel': ['linear'],\n","    'clf__C': [0.1, 1, 10, 100],\n","}, \n","{\n","    'clf__kernel': ['rbf'],\n","    'clf__C': [0.01, 0.1, 1, 10, 100],\n","    'clf__gamma': [0.01, 0.1, 1, 10, 100],\n","}]\n","\n","RANDOM_FOREST_PARAMS = {\n","    'clf__max_depth': [25, 50, 75],\n","    'clf__max_features': [\"sqrt\", \"log2\"], # same as auto\n","    'clf__criterion': ['gini', 'entropy'],\n","    'clf__n_estimators': [100, 300, 500, 1000]\n","}\n","\n","KNN_PARAMS = {\n","    'clf__n_neighbors': [5, 15, 25, 35, 45, 55, 65],\n","    'clf__weights': ['uniform', 'distance'],\n","    'clf__p': [1, 2, 10]\n","}\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["# Introduction"],"metadata":{}},{"source":["import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV, learning_curve\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import log_loss, confusion_matrix, roc_curve, auc, roc_auc_score\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","import plotly\n","import plotly.plotly as py\n","from plotly.plotly import plot, iplot\n","import plotly.graph_objs as go\n","import plotly.figure_factory as ff\n","\n","from scipy.cluster import hierarchy as hc\n","\n","from imblearn.pipeline import make_pipeline, Pipeline\n","from imblearn.over_sampling import SMOTE\n","\n","from prettytable import PrettyTable\n","from functools import wraps\n","import time\n","\n","plotly.tools.set_credentials_file(username='modusV', api_key='OBKKnTR2vYTeKIOKtRU6')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","# Wrapper to calculate functions speed\n","\n","def watcher(func):\n","    \"\"\"\n","    Decorator for dumpers.\n","    Shows how much time it\n","    takes to create/retrieve\n","    the blob.\n","    \"\"\"\n","    @wraps(func)\n","    def wrapper(*args, **kwargs):\n","        start = time.perf_counter()\n","        result = func(*args, **kwargs)\n","        end = time.perf_counter()\n","        print(f\" ===> took {end-start} seconds\")\n","        return result\n","    return wrapper\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["# Dataset analysis and preprocessing"],"metadata":{}},{"source":["# Load the dataset\n","dataset = pd.read_csv(\"./Input/mushrooms.csv\")\n","# dataset = pd.read_csv(\"./mushrooms.csv\")\n","print(\"The dataset has %d rows and %d columns.\" % dataset.shape)"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["# Count number of classes for classification\n","print(f\"There are {dataset['class'].unique().size} different classes:\"\n","      f\"\\n {dataset['class'].unique().tolist()}\")\n","\n","# Count number of unique data for every column\n","print(f\"Unique values for every field: \\n{dataset.nunique()}\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# See data types \n","print(f\"Data types: \\n{dataset.head(5)}\")\n","\n","# All columns \n","print(\", \".join(str(a) for a in dataset.columns))"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{}},{"source":["n_columns_original = len(dataset.columns)\n","to_drop = [col for col in dataset.columns if dataset[col].nunique() == 1]\n","dataset.drop(to_drop, axis=1, inplace=True)\n","\n","print(f\"{n_columns_original - len(dataset.columns)} not significant columns have been removed\")\n","\n","\n","#### Handling missing values"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Check if any field is null\n","if dataset.isnull().any().any():\n","    print(\"There are some null values\")\n","else:\n","    print(\"There are no null values\")"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["print(\"There are \" + str((dataset['stalk-root'] == \"?\").sum()) + \" missing values in stalk-root column\")\n","# df_drop = dataset[dataset['stalk-root'] != \"?\"]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["def preprocess(dataset):\n","    mapping = {}  \n","    d = dataset.copy()\n","    labelEncoder = LabelEncoder()\n","    for column in dataset.columns:\n","        labelEncoder.fit(dataset[column])\n","        mapping[column] = dict(zip(labelEncoder.classes_, labelEncoder.transform(labelEncoder.classes_)))\n","        d[column] = labelEncoder.transform(dataset[column])\n","        \n","    return d, labelEncoder, mapping\n","\n","le = 0\n","pre_data, l_encoder, le_mapping = preprocess(dataset)\n","\n","# Check mapping\n","print(le_mapping)\n","\n","# Check new data\n","pre_data.head(5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["# Check new labels\n","print(pre_data.groupby('class').size())"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["# Get insights on the dataset\n","dataset.describe()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["y = dataset[\"class\"].value_counts()\n","print(y)\n","class_dict = [\"edible\", \"poisonous\"]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["data = [go.Bar(\n","            x=class_dict,\n","            y=y,\n","            marker=dict(\n","            color=PLOTLY_COLORS),\n","            opacity=PLOTLY_OPACITY,\n","    )]\n","\n","layout = go.Layout(title=\"Class distribution\",\n","                   autosize=False,\n","                   width=400,\n","                   height=400,\n","                   yaxis=dict(\n","                        title='N. samples',\n","                    ),\n","                   )\n","fig = go.Figure(data=data, layout=layout)\n","py.iplot(fig, filename='color-bar')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["\n","def create_box(type, data, col, visible=False):\n","    if type == \"edible\":\n","        c = PLOTLY_COLORS[0]\n","    else:\n","        c = PLOTLY_COLORS[1]\n","    return go.Box(\n","        y = data[col],\n","        name = type,\n","        marker=dict(color = c),\n","        visible=visible,\n","        opacity=PLOTLY_OPACITY,\n","    )\n","\n","edible = pre_data[pre_data[\"class\"] == 0]\n","poisonous = pre_data[pre_data[\"class\"] == 1]\n","box_features = [col for col in pre_data.columns if ((col != 'class') and (dataset[col].nunique() > 5))]\n","\n","active_index = 0\n","box_edible = [(create_box(\"edible\", edible, col, False) if i != active_index \n","               else create_box(\"edible\", edible, col, True)) \n","              for i, col in enumerate(box_features)]\n","\n","box_poisonous = [(create_box(\"poisonous\", poisonous, col, False) if i != active_index \n","               else create_box(\"poisonous\", poisonous, col, True)) \n","              for i, col in enumerate(box_features)]\n","\n","data = box_edible + box_poisonous\n","n_features = len(box_features)\n","steps = []\n","\n","for i in range(n_features):\n","    step = dict(\n","        method = 'restyle',  \n","        args = ['visible', [False] * len(data)],\n","        label = box_features[i],\n","    )\n","    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n","    step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n","    steps.append(step)\n","    \n","sliders = [dict(\n","    active = active_index,\n","    currentvalue = dict(\n","        prefix = \"Feature: \", \n","        xanchor= 'center',\n","    ),\n","    pad = {\"t\": 50},\n","    steps = steps,\n","    len=1,\n",")]\n","\n","layout = dict(\n","    sliders=sliders,\n","    yaxis=dict(\n","        title='value',\n","        automargin=True,\n","    ),\n","    legend=dict(\n","        x=0,\n","        y=1,\n","    ),\n",")\n","\n","fig = dict(data=data, layout=layout)\n","py.iplot(fig, filename='box_slider')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["correlation_matrix = pre_data.corr(method='pearson')\n","\n","trace = go.Heatmap(\n","    z=correlation_matrix.values.tolist(), \n","    x=correlation_matrix.columns, \n","    y=correlation_matrix.columns, \n","    colorscale=COLORSCALE_HEATMAP,\n","    opacity=0.95,\n","    zmin=-1,\n","    zmax=1)\n","    \n","\n","data=[trace]\n","\n","layout = go.Layout(\n","    title='Heatmap of columns correlation',\n","    autosize=False,\n","    width=850,\n","    height=700,\n","    yaxis=go.layout.YAxis(automargin=True),\n","    xaxis=dict(tickangle=40),\n","    margin=go.layout.Margin(l=0, r=200, b=200, t=80)\n",")\n","\n","fig = go.Figure(data=data, layout=layout)\n","py.iplot(fig, filename='labelled-heatmap4')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["\n","def create_hist(type, data, col, visible=False):\n","    if type == \"edible\":\n","        c = PLOTLY_COLORS[0]\n","    else:\n","        c = PLOTLY_COLORS[1]\n","    return go.Histogram(\n","        x = data[col],\n","        name = type,\n","        marker=dict(color = c),\n","        visible=visible,\n","        opacity=PLOTLY_OPACITY,\n","    )\n","\n","hist_features = [col for col in pre_data.columns if (col != 'class')]\n","\n","active_index = 0\n","hist_edible = [(create_hist(\"edible\", edible, col, False) if i != active_index \n","               else create_hist(\"edible\", edible, col, True)) \n","              for i, col in enumerate(hist_features)]\n","\n","hist_poisonous = [(create_hist(\"poisonous\", poisonous, col, False) if i != active_index \n","               else create_hist(\"poisonous\", poisonous, col, True)) \n","              for i, col in enumerate(hist_features)]\n","\n","total_data = hist_edible + hist_poisonous\n","n_features = len(hist_features)\n","steps = []\n","\n","for i in range(n_features):\n","    step = dict(\n","        method = 'restyle',  \n","        args = ['visible', [False] * len(total_data)],\n","        label = hist_features[i],\n","    )\n","    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n","    step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n","    steps.append(step)\n","    \n","sliders = [dict(\n","    active = active_index,\n","    currentvalue = dict(\n","        prefix = \"Feature: \", \n","        xanchor= 'center',\n","    ),\n","    pad = {\"t\": 50},\n","    steps = steps,\n",")]\n","\n","layout = dict(\n","    sliders=sliders,\n","    yaxis=dict(\n","        title='value',\n","        automargin=True,\n","    ),\n","    legend=dict(\n","        x=0,\n","        y=1,\n","    ),\n","    barmode='group',\n","    bargap=0.15,\n","    bargroupgap=0.1\n",")\n","\n","fig = dict(data=total_data, layout=layout)\n","py.iplot(fig, filename='hist_slider')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["\n","X = np.random.rand(10, 10)\n","names = pre_data.columns\n","inverse_correlation = 1 - abs(pre_data.corr()) # This is the 'dissimilarity' method\n","fig = ff.create_dendrogram(inverse_correlation.values, \n","                           labels=names, \n","                           colorscale=PLOTLY_COLORS, \n","                           linkagefun=lambda x: hc.linkage(x, 'average'))\n","\n","fig['layout'].update(dict(\n","    title=\"Dendrogram of correlation among features\",\n","    width=800, \n","    height=600,\n","    xaxis=dict(\n","        title='Features',\n","    ),\n","    yaxis=dict(\n","        title='Distance',\n","        \n","    ),\n","))\n","iplot(fig, filename='dendrogram_corr_clustering')\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","drop_data = pre_data[pre_data['stalk-root'] != le_mapping['stalk-root']['?']]\n","\n","y_data = pre_data['class']\n","y_drop_data = drop_data['class']\n","\n","X_pre_data = pre_data.drop(['class'], axis=1)\n","X_drop_data = drop_data.drop(['class'], axis=1)\n","\n","scaler = StandardScaler()\n","\n","X_scaled_data = scaler.fit_transform(X_pre_data)\n","X_scaled_drop_data = scaler.fit_transform(X_drop_data)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["\n","from sklearn.decomposition import PCA\n","\n","\n","pca = PCA(random_state=RANDOM_SEED)\n","projected_data = pca.fit_transform(X_scaled_data)\n","\n","tot_var = np.sum(pca.explained_variance_)\n","ex_var = [(i / tot_var) * 100 for i in sorted(pca.explained_variance_, reverse=True)]\n","cum_ex_var = np.cumsum(ex_var)\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["cum_var_bar = go.Bar(\n","    x=list(range(1, len(cum_ex_var) + 1)), \n","    y=ex_var,\n","    name=\"Variance of each component\",\n","    marker=dict(\n","        color=PLOTLY_COLORS[0],\n","    ),\n","    opacity=PLOTLY_OPACITY\n",")\n","variance_line = go.Scatter(\n","    x=list(range(1, len(cum_ex_var) + 1)),\n","    y=cum_ex_var,\n","    mode='lines+markers',\n","    name=\"Cumulative variance\",\n","    marker=dict(\n","        color=PLOTLY_COLORS[1],\n","    ),\n","    opacity=PLOTLY_OPACITY,\n","    line=dict(\n","        shape='hv',\n","    ))\n","data = [cum_var_bar, variance_line]\n","layout = go.Layout(\n","    title='Individual and Cumulative Explained Variance',\n","    autosize=True,\n","    yaxis=dict(\n","        title='Explained variance (%)',\n","    ),\n","    xaxis=dict(\n","        title=\"Principal components\",\n","        dtick=1,\n","    ),\n","    legend=dict(\n","        x=0,\n","        y=1,\n","    ),\n",")\n","fig = go.Figure(data=data, layout=layout)\n","iplot(fig, filename='basic-bar')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["\n","n_comp = 9\n","pca.components_ = pca.components_[:n_comp]\n","reduced_data = np.dot(projected_data, pca.components_.T)\n","# pca.inverse_transform(projected_data)\n","X_df_reduced = pd.DataFrame(reduced_data, columns=[\"PC#%d\" % (x + 1) for x in range(n_comp)])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","from sklearn.cluster import KMeans\n","\n","N=pre_data.values\n","pca = PCA(n_components=2)\n","x = pca.fit_transform(N)\n","\n","kmeans = KMeans(n_clusters=2, random_state=RANDOM_SEED)\n","X_clustered = kmeans.fit_predict(N)\n","print(kmeans.score)\n","\n","ed_idx = np.where(X_clustered == 0)\n","po_idx = np.where(X_clustered == 1)\n","\n","p1 = go.Scatter(\n","    x=np.take(x[:,0], indices=ed_idx)[0],\n","    y=np.take(x[:,1], indices=ed_idx)[0],\n","    mode='markers',\n","    name=\"Edible\",\n","    marker=dict(\n","        color=PLOTLY_COLORS[0],\n","    ),\n","    opacity=PLOTLY_OPACITY)\n","\n","p2 = go.Scatter(\n","    x=np.take(x[:,0], indices=po_idx)[0],\n","    y=np.take(x[:,1], indices=po_idx)[0],\n","    mode='markers',\n","    name=\"Poisonous\",\n","    marker=dict(\n","        color=PLOTLY_COLORS[1],\n","    ),\n","    opacity=PLOTLY_OPACITY)\n","    \n","\n","data = [p1, p2]\n","\n","layout = go.Layout(\n","    title='Data clustered using first two components',\n","    autosize=True,\n","    yaxis=dict(\n","        title='Second component',\n","    ),\n","    xaxis=dict(\n","        title=\"First component\",\n","        dtick=1,\n","    ),\n","    legend=dict(\n","        x=0,\n","        y=1,\n","    ),\n",")\n","fig = go.Figure(data=data, layout=layout)\n","iplot(fig, filename='clusters-scatter')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"cell_type":"markdown","source":["# Classification"],"metadata":{}},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled_data, y_data, test_size=0.2, random_state=RANDOM_SEED)\n","X_train_pc, X_test_pc, y_train_pc, y_test_pc = train_test_split(X_df_reduced, y_data, test_size=0.2, random_state=RANDOM_SEED)\n","X_train_drop, X_test_drop, y_train_drop, y_test_drop = train_test_split(X_scaled_drop_data, y_drop_data, test_size=0.2, random_state=RANDOM_SEED)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["\n","def print_gridcv_scores(grid_search, n=5):\n","    \n","    if not hasattr(grid_search, 'best_score_'):\n","        raise KeyError('grid_search is not fitted.')\n","    \n","    t = PrettyTable()\n","\n","    print(\"Best grid scores on validation set:\")\n","    indexes = np.argsort(grid_search.cv_results_['mean_test_score'])[::-1][:n]\n","    means = grid_search.cv_results_['mean_test_score'][indexes]\n","    stds = grid_search.cv_results_['std_test_score'][indexes]\n","    params = np.array(grid_search.cv_results_['params'])[indexes]\n","    \n","    t.field_names = ['Score'] + [f for f in params[0].keys()] \n","    for mean, std, params in zip(means, stds, params):\n","        row=[\"%0.3f (+/-%0.03f)\" % (mean, std * 2)] + [p for p in params.values()]\n","        t.add_row(row)\n","    print(t)\n","               \n","@watcher\n","def param_tune_grid_cv(clf, params, X_train, y_train, cv):\n","    pipeline = Pipeline([('clf', clf)])\n","    grid_search = GridSearchCV(estimator=pipeline, \n","                               param_grid=params, \n","                               cv=cv, \n","                               n_jobs=-1,       # Use all processors\n","                               scoring='f1',    # Use f1 metric for evaluation\n","                               return_train_score=True)\n","    grid_search.fit(X_train, y_train)\n","    return grid_search\n","   \n","\n","def score(clfs, datasets):\n","    scores = []\n","    for c, (X_test, y_test) in zip(clfs, datasets):\n","        scores.append(c.score(X_test, y_test))\n","\n","    return scores\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["def hexToRGBA(hex, alpha):\n","    r = int(hex[1:3], 16)\n","    g = int(hex[3:5], 16)\n","    b = int(hex[5:], 16)\n","\n","    if alpha:\n","        return \"rgba(\" + str(r) + \", \" + str(g) + \", \" + str(b) + \", \" + str(alpha) + \")\"\n","    else:\n","        return \"rgb(\" + str(r) + \", \" + str(g) + \", \" + str(b) + \")\"\n","\n","\n","# partially based on https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n","def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=-1, train_sizes=np.linspace(.008, 1.0, 5)):\n","    \"\"\"\n","    Generate a simple plot of the test and training learning curve.\n","\n","    Parameters\n","    ----------\n","    estimator : object type that implements the \"fit\" and \"predict\" methods\n","        An object of that type which is cloned for each validation.\n","\n","    title : string\n","        Title for the chart.\n","\n","    X : array-like, shape (n_samples, n_features)\n","        Training vector, where n_samples is the number of samples and\n","        n_features is the number of features.\n","\n","    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n","        Target relative to X for classification or regression;\n","        None for unsupervised learning.\n","\n","    cv : int, cross-validation generator or an iterable, optional\n","        Determines the cross-validation splitting strategy.\n","        Possible inputs for cv are:\n","          - None, to use the default 3-fold cross-validation,\n","          - integer, to specify the number of folds.\n","          - :term:`CV splitter`,\n","          - An iterable yielding (train, test) splits as arrays of indices.\n","\n","        For integer/None inputs, if ``y`` is binary or multiclass,\n","        :class:`StratifiedKFold` used. If the estimator is not a classifier\n","        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n","\n","        Refer :ref:`User Guide <cross_validation>` for the various\n","        cross-validators that can be used here.\n","\n","    n_jobs : int or None, optional (default=None)\n","        Number of jobs to run in parallel.\n","        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n","        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n","        for more details.\n","\n","    train_sizes : array-like, shape (n_ticks,), dtype float or int\n","        Relative or absolute numbers of training examples that will be used to\n","        generate the learning curve. If the dtype is float, it is regarded as a\n","        fraction of the maximum size of the training set (that is determined\n","        by the selected validation method), i.e. it has to be within (0, 1].\n","        Otherwise it is interpreted as absolute sizes of the training sets.\n","        Note that for classification the number of samples usually have to\n","        be big enough to contain at least one sample from each class.\n","        (default: np.linspace(0.1, 1.0, 5))\n","    \"\"\"\n","    \n","    train_sizes, train_scores, test_scores = learning_curve(\n","        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=\"f1\", random_state=RANDOM_SEED)\n","    train_scores_mean = np.mean(train_scores, axis=1)\n","    train_scores_std = np.std(train_scores, axis=1)\n","    test_scores_mean = np.mean(test_scores, axis=1)\n","    test_scores_std = np.std(test_scores, axis=1)\n","    \n","    trace1 = go.Scatter(\n","        x=train_sizes, \n","        y=train_scores_mean - train_scores_std, \n","        showlegend=False,\n","        mode=\"lines\",\n","        name=\"\",\n","        hoverlabel = dict(\n","            namelength=20\n","        ),\n","        line = dict(\n","            width = 0.1,\n","            color = hexToRGBA(PLOTLY_COLORS[0], 0.4),\n","        ),\n","    )\n","    trace2 = go.Scatter(\n","        x=train_sizes, \n","        y=train_scores_mean + train_scores_std, \n","        showlegend=False,\n","        fill=\"tonexty\",\n","        mode=\"lines\",\n","        name=\"\",\n","        hoverlabel = dict(\n","            namelength=20\n","        ),\n","        line = dict(\n","            width = 0.1,\n","            color = hexToRGBA(PLOTLY_COLORS[0], 0.4),\n","        ),\n","    )\n","    trace3 = go.Scatter(\n","        x=train_sizes, \n","        y=train_scores_mean, \n","        showlegend=True,\n","        name=\"Train score\",\n","        line = dict(\n","            color = PLOTLY_COLORS[0],\n","        ),\n","    )\n","    \n","    trace4 = go.Scatter(\n","        x=train_sizes, \n","        y=test_scores_mean - test_scores_std, \n","        showlegend=False,\n","        mode=\"lines\",\n","        name=\"\",\n","        hoverlabel = dict(\n","            namelength=20\n","        ),\n","        line = dict(\n","            width = 0.1,\n","            color = hexToRGBA(PLOTLY_COLORS[1], 0.4),\n","        ),\n","    )\n","    trace5 = go.Scatter(\n","        x=train_sizes, \n","        y=test_scores_mean + test_scores_std, \n","        showlegend=False,\n","        fill=\"tonexty\",\n","        mode=\"lines\",\n","        name=\"\",\n","        hoverlabel = dict(\n","            namelength=20\n","        ),\n","        line = dict(\n","            width = 0.1,\n","            color = hexToRGBA(PLOTLY_COLORS[1], 0.4),\n","        ),\n","    )\n","    trace6 = go.Scatter(\n","        x=train_sizes, \n","        y=test_scores_mean, \n","        showlegend=True,\n","        name=\"Test score\",\n","        line = dict(\n","            color = PLOTLY_COLORS[1],\n","        ),\n","    )\n","    \n","    data = [trace1, trace2, trace3, trace4, trace5, trace6]\n","    layout = go.Layout(\n","        title=title,\n","        autosize=True,\n","        yaxis=dict(\n","            title='F1 Score',\n","        ),\n","        xaxis=dict(\n","            title=\"#Training samples\",\n","        ),\n","        legend=dict(\n","            x=0.8,\n","            y=0,\n","        ),\n","    )\n","    fig = go.Figure(data=data, layout=layout)\n","    return iplot(fig, filename=title)\n","\n","\n","def print_confusion_matrix(gs, X_test, y_test):\n","\n","    gs_score = gs.score(X_test, y_test)\n","    y_pred = gs.predict(X_test)\n","\n","    cm = confusion_matrix(y_test, y_pred)\n","    t = PrettyTable()\n","    t.add_row([\"True Edible\", cm[0][0], cm[0][1]])\n","    t.add_row([\"True Poisonous\", cm[1][0], cm[1][1]])\n","    t.field_names = [\" \", \"Predicted Edible\", \"Predicted Poisonous\"]\n","    print(t)\n","\n","    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix\n","    cm_df = pd.DataFrame(cm.round(3), index=[\"True edible\", \"True Poisonous\"], columns=[\"Predicted edible\", \"Predicted poisonous\"])\n","    cm_df\n","\n","\n","def print_raw_score(clf, X_test, y_test):\n","    print(\"Score achieved by NB: %0.3f\" % (score([clf], [(X_test, y_test)])[0]))\n","\n","\n","def plot_feature_importance(feature_importance, title):\n","    trace1 = go.Bar(\n","        x=feature_importance[:, 0],\n","        y=feature_importance[:, 1],\n","        marker = dict(color = PLOTLY_COLORS[0]),\n","        opacity=PLOTLY_OPACITY,\n","        name='Feature importance'\n","    )\n","    data = [trace1]\n","    layout = go.Layout(\n","        title=title,\n","        autosize=True,\n","        margin=go.layout.Margin(l=50, r=100, b=150),\n","        xaxis=dict(\n","            title='feature',\n","            tickangle=30\n","        ),\n","        yaxis=dict(\n","            title='feature importance',\n","            automargin=True,\n","        ),\n","    )\n","    fig = go.Figure(data=data, layout=layout)\n","    return iplot(fig, filename=title)"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["kf = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED)\n","clf_lr = LogisticRegression(random_state=RANDOM_SEED)\n","clf_lr_balanced = LogisticRegression(random_state=RANDOM_SEED, class_weight=\"balanced\")\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","print(\"Full dataset cv:\")\n","gs_full = param_tune_grid_cv(clf_lr, LOGISTIC_REGRESSION_PARAMS, X_train, y_train, kf)\n","print(\"\\nDataset projected on first 9 pc cv:\")\n","gs_pc = param_tune_grid_cv(clf_lr, LOGISTIC_REGRESSION_PARAMS, X_train_pc, y_train_pc, kf)\n","print(\"\\nFull dataset with dropped values took:\")\n","gs_drop = param_tune_grid_cv(clf_lr, LOGISTIC_REGRESSION_PARAMS, X_train_drop, y_train_drop, kf)\n","gss = [gs_full, gs_pc, gs_drop]\n","\n","test_results = score(gss, [(X_test, y_test), (X_test_pc, y_test_pc), (X_test_drop, y_test_drop)])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","print(\"Full dataset cv:\")\n","gs_full_balanced = param_tune_grid_cv(clf_lr_balanced, LOGISTIC_REGRESSION_PARAMS, X_train, y_train, kf)\n","print(\"\\nDataset projected on first 9 pc cv:\")\n","gs_pc_balanced = param_tune_grid_cv(clf_lr_balanced, LOGISTIC_REGRESSION_PARAMS, X_train_pc, y_train_pc, kf)\n","print(\"\\nFull dataset with dropped values took:\")\n","gs_drop_balanced = param_tune_grid_cv(clf_lr_balanced, LOGISTIC_REGRESSION_PARAMS, X_train_drop, y_train_drop, kf)\n","gss_balanced = [gs_full_balanced, gs_pc_balanced, gs_drop_balanced]\n","\n","test_results_balanced = score(gss_balanced, [(X_test, y_test), (X_test_pc, y_test_pc), (X_test_drop, y_test_drop)])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["X_train.shape\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["dataset_strings = [\"full dataset\", \"dataset with first 9 principal components\", \"dataset with dropped missing values\"]\n","method_strings = [\"without any balancing\", \"using balanced class weights\"]\n","\n","t = PrettyTable()\n","t.field_names = [\"Score\", \"Dataset\", \"Type\"]\n","\n","result_row = []\n","for ms, results in zip(method_strings, [test_results, test_results_balanced]):\n","    for ds, res in zip(dataset_strings, results):\n","        result_row.append([\"%.3f\" % res, ds, ms])\n","        \n","result_row = sorted(result_row, key=lambda kv: kv[0], reverse=True)\n","\n","for k in result_row:\n","    t.add_row(k)\n","\n","t.title = \"F1 score  dataset and method\"\n","print(t)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["print_gridcv_scores(gs_drop)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print_confusion_matrix(gs_drop, X_test_drop, y_test_drop)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plot_learning_curve(gs_drop.best_estimator_, \"Learning Curve of Logistic Regression\", \n","                    np.concatenate((X_train_drop, X_test_drop)),\n","                    np.concatenate((y_train_drop, y_test_drop)), \n","                    cv=5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["## Support vector machine"],"metadata":{}},{"source":["clf_svm = SVC(probability=True, random_state=RANDOM_SEED)\n","gs_pc_svm = param_tune_grid_cv(clf_svm, SVM_PARAMS, X_train_pc, y_train_pc, kf)\n","print_gridcv_scores(gs_pc_svm, n=5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plot_learning_curve(gs_pc_svm.best_estimator_, \"Learning curve of SVM\", \n","                    np.concatenate((X_train_pc, X_test_pc)),\n","                    np.concatenate((y_train_pc, y_test_pc)),\n","                    cv=5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print_confusion_matrix(gs_pc_svm, X_test_pc, y_test_pc)"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["clf_nb = GaussianNB()\n","clf_nb.fit(X_train, y_train)\n","print_raw_score(clf_nb, X_test, y_test)\n","print_confusion_matrix(clf_nb, X_test, y_test)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plot_learning_curve(clf_nb, \"Learning curve of GaussianNB\", \n","                    np.concatenate((X_train, X_test), axis=0), \n","                    np.concatenate((y_train, y_test), axis=0), \n","                    cv=5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","clf_pc_rf = RandomForestClassifier(random_state=RANDOM_SEED)\n","gs_pc_rf = param_tune_grid_cv(clf_pc_rf, RANDOM_FOREST_PARAMS, X_train_pc, y_train_pc, kf)\n","print_gridcv_scores(gs_pc_rf, n = 5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print_confusion_matrix(gs_pc_rf, X_test_pc, y_test_pc)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plot_learning_curve(gs_pc_rf.best_estimator_, \"Learning curve of Random Forest Classifier\", \n","                    np.concatenate((X_train_pc, X_test_pc)),\n","                    np.concatenate((y_train_pc, y_test_pc)), \n","                    cv=5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["feature_importance = np.array(  sorted(zip(X_train_pc.columns, \n","                                gs_pc_rf.best_estimator_.named_steps['clf'].feature_importances_),\n","                                key=lambda x: x[1], reverse=True))\n","plot_feature_importance(feature_importance, \"Feature importance in the random forest\")\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["'''\n","print(\"Full dataset cv:\")\n","gs_full = param_tune_grid_cv(clf_svm, SVM_PARAMS, X_train, y_train, kf)\n","print(\"\\nDataset projected on first 9 pc cv:\")\n","gs_pc = param_tune_grid_cv(clf_svm, SVM_PARAMS, X_train_pc, y_train_pc, kf)\n","print(\"\\nFull dataset with dropped values took:\")\n","gs_drop = param_tune_grid_cv(clf_svm, SVM_PARAMS, X_train_drop, y_train_drop, kf)\n","gss = [gs_full, gs_pc, gs_drop]\n","\n","test_results = score(gss, [(X_test, y_test), (X_test_pc, y_test_pc), (X_test_drop, y_test_drop)])\n","'''"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","clf_knn = KNeighborsClassifier()\n","gs_knn = param_tune_grid_cv(clf_knn, KNN_PARAMS, X_train_pc, y_train_pc, kf)\n","print_gridcv_scores(gs_knn, n=5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print_confusion_matrix(gs_knn, X_train_pc, y_train_pc)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","plot_learning_curve(gs_knn.best_estimator_, \"Learning curve of Random Forest Classifier\", \n","                    np.concatenate((X_train_pc, X_test_pc)),\n","                    np.concatenate((y_train_pc, y_test_pc)), \n","                    cv=5)\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","def plot_roc_curve(classifiers, legend, title, X_test, y_test):\n","    t1 = go.Scatter(\n","        x=[0, 1], \n","        y=[0, 1], \n","        showlegend=False,\n","        mode=\"lines\",\n","        name=\"\",\n","        line = dict(\n","            color = COLOR_PALETTE[0],\n","        ),\n","    )\n","    \n","    data = [t1]\n","    aucs = []\n","    for clf, string, c in zip(classifiers, legend, COLOR_PALETTE[1:]):\n","        y_test_roc = np.array([([0, 1] if y else [1, 0]) for y in y_test])\n","        y_score = clf.predict_proba(X_test)\n","        \n","        # Compute ROC curve and ROC area for each class\n","        fpr = dict()\n","        tpr = dict()\n","        roc_auc = dict()\n","        for i in range(2):\n","            fpr[i], tpr[i], _ = roc_curve(y_test_roc[:, i], y_score[:, i])\n","            roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","        # Compute micro-average ROC curve and ROC area\n","        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_roc.ravel(), y_score.ravel())\n","        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","        aucs.append(roc_auc['micro'])\n","\n","        trace = go.Scatter(\n","            x=fpr['micro'], \n","            y=tpr['micro'], \n","            showlegend=True,\n","            mode=\"lines\",\n","            name=string + \" (area = %0.2f)\" % roc_auc['micro'],\n","            hoverlabel = dict(\n","                namelength=30\n","            ),\n","            line = dict(\n","                color = c,\n","            ),\n","        )\n","        data.append(trace)\n","\n","    layout = go.Layout(\n","        title=title,\n","        autosize=False,\n","        width=550,\n","        height=550,\n","        yaxis=dict(\n","            title='True Positive Rate',\n","        ),\n","        xaxis=dict(\n","            title=\"False Positive Rate\",\n","        ),\n","        legend=dict(\n","            x=0.4,\n","            y=0.06,\n","        ),\n","    )\n","    fig = go.Figure(data=data, layout=layout)\n","    return aucs, iplot(fig, filename=title)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","classifiers = [gs_drop, gs_pc_svm, clf_nb, gs_pc_rf, gs_knn]\n","classifier_names = [\"Logistic Regression\", \"SVM\", \"GaussianNB\", \"Random Forest\", \"KNN\"]\n","auc_scores, roc_plot = plot_roc_curve(classifiers, classifier_names, \"ROC curve\", X_test, y_test)\n","roc_plot\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}