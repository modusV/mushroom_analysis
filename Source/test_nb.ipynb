{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\nimport os\ntry:\n\tos.chdir(os.path.join(os.getcwd(), '..'))\n\tprint(os.getcwd())\nexcept:\n\tpass\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Safe to eat or deadly poisonous?\n","## An analysis on mushroom classification by Lorenzo Santolini"],"metadata":{}},{"cell_type":"markdown","source":["### Code snippet for google colab"],"metadata":{}},{"source":["# Little code snippet to import on Google Colab the dataset\n","'''\n","!pip install -U -q kaggle\n","!mkdir -p ~/.kaggle\n","\n","# Insert here your kaggle API key\n","from google.colab import files\n","files.upload()\n","\n","!cp kaggle.json ~/.kaggle/\n","!kaggle datasets download -d uciml/mushroom-classification\n","!unzip mushroom-classification.zip\n","!ls\n","'''\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Define all the constants that will be used\n","\n","PLOTLY_COLORS = ['#140DFF', '#FF0DE2']\n","COLOR_PALETTE = ['#140DFF', '#FF0DE2', '#CAFFD0', '#C9E4E7', '#B4A0E5', '#904C77']\n","COLORSCALE_HEATMAP = [         [0.0, 'rgb(70,0,252)'], \n","                [0.1111111111111111, 'rgb(78,0,252)'], \n","                [0.2222222222222222, 'rgb(90,0,252)'], \n","                [0.3333333333333333, 'rgb(110,0,248)'], \n","                [0.4444444444444444, 'rgb(130,0,238)'], \n","                [0.5555555555555556, 'rgb(145,0,228)'], \n","                [0.6666666666666666, 'rgb(166,0,218)'], \n","                [0.7777777777777778, 'rgb(187,0,213)'], \n","                [0.8888888888888888, 'rgb(200,0,202)'], \n","                               [1.0, 'rgb(210,0,191)']]\n","PLOTLY_OPACITY = 0.7\n","RANDOM_SEED = 11\n","\n","LOGISTIC_REGRESSION_PARAMS = {\n","    'clf__solver': ['liblinear'],  # best for small datasets\n","    'clf__C': [0.01, 0.1, 1, 10, 100], # smaller value, stronger regularization, like svm\n","    'clf__penalty': ['l2', 'l1']\n","}\n","\n","SVM_PARAMS = [\n","{\n","    'clf__kernel': ['linear'],\n","    'clf__C': [0.1, 1, 10, 100],\n","}, \n","{\n","    'clf__kernel': ['rbf'],\n","    'clf__C': [0.01, 0.1, 1, 10, 100],\n","    'clf__gamma': [0.01, 0.1, 1, 10, 100],\n","}]\n","\n","RANDOM_FOREST_PARAMS = {\n","    'clf__max_depth': [25, 50, 75],\n","    'clf__max_features': [\"sqrt\", \"log2\"], # sqrt is the same as auto\n","    'clf__criterion': ['gini', 'entropy'],\n","    'clf__n_estimators': [100, 300, 500, 1000]\n","}\n","\n","KNN_PARAMS = {\n","    'clf__n_neighbors': [5, 15, 25, 35, 45, 55, 65],\n","    'clf__weights': ['uniform', 'distance'],\n","    'clf__p': [1, 2, 10]\n","}\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["# Introduction"],"metadata":{}},{"source":["# Import all the libraries\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV, learning_curve\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import log_loss, confusion_matrix, roc_curve, auc, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","\n","import plotly\n","import plotly.plotly as py\n","from plotly.plotly import plot, iplot\n","import plotly.graph_objs as go\n","import plotly.figure_factory as ff\n","\n","from scipy.cluster import hierarchy as hc\n","import scipy.spatial as scs\n","\n","from imblearn.pipeline import make_pipeline, Pipeline\n","from imblearn.over_sampling import SMOTE\n","\n","from prettytable import PrettyTable\n","from functools import wraps\n","import time\n","\n","plotly.tools.set_credentials_file(username='modusV', api_key='OBKKnTR2vYTeKIOKtRU6')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","# Wrapper to calculate functions speed\n","\n","def watcher(func):\n","    \"\"\"\n","    Decorator for dumpers.\n","    Shows how much time it\n","    takes to create/retrieve\n","    the blob.\n","    \"\"\"\n","    @wraps(func)\n","    def wrapper(*args, **kwargs):\n","        start = time.perf_counter()\n","        result = func(*args, **kwargs)\n","        end = time.perf_counter()\n","        print(f\" ===> took {end-start} seconds\")\n","        return result\n","    return wrapper\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","# Define classes \n","\n","class Dataset:\n","    \n","    def __init__(self, data, seed, name):\n","        self.dataset = data\n","        self.seed = seed\n","        self.name = name\n","        \n","    def set_name(name):\n","        self.name = name\n","    \n","    def get_name(name):\n","        return self.name\n","\n","    def import_data(path):\n","        self.dataset = pd.read_csv(path)\n","\n","    def count_classes():\n","        self.n_classes = self.dataset['class'].unique().size\n","        print(f\"There are {self.n_classes} different classes:\"\n","              f\"\\n {self.dataset['class'].unique().tolist()}\")\n","\n","\n","\n","class Classifier:\n","\n","    def __init__(self, classifier, params, dataset, seed, name):\n","        self.classifier = classifier\n","        self.params = params\n","        self.dataset = dataset\n","        self.seed = seed\n","        self.name = name\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["# Dataset load and overall view"],"metadata":{}},{"source":["# Load the dataset\n","dataset = pd.read_csv(\"./Input/mushrooms.csv\")\n","# dataset = pd.read_csv(\"./mushrooms.csv\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# Shape of the dataset\n","print(\"The dataset has %d rows and %d columns.\" % dataset.shape)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["# Count number of classes for classification\n","print(f\"There are {dataset['class'].unique().size} different classes:\"\n","      f\"\\n {dataset['class'].unique().tolist()}\")\n","\n","# Count number of unique data for every column\n","print(f\"Unique values for every field: \\n{dataset.nunique()}\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{}},{"cell_type":"markdown","source":["## 1 - Check data types"],"metadata":{}},{"source":["# See data types \n","print(f\"Data types: \\n{dataset.head(5)}\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"cell_type":"markdown","source":["## 2 - Remove any not significant column"],"metadata":{}},{"source":["n_columns_original = len(dataset.columns)\n","to_drop = [col for col in dataset.columns if dataset[col].nunique() == 1]\n","dataset.drop(to_drop, axis=1, inplace=True)\n","\n","for d in to_drop:\n","    print(str(d) + \" \", end=\"\")\n","print(\"have been removed because zero variance\")\n","print(f\"{n_columns_original - len(dataset.columns)} not significant columns have been removed\")\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"cell_type":"markdown","source":["## 3 - Handling missing values"],"metadata":{}},{"source":["# Check if any field is null\n","if dataset.isnull().any().any():\n","    print(\"There are some null values\")\n","else:\n","    print(\"There are no null values\")"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["print(\"There are \" + str((dataset['stalk-root'] == \"?\").sum()) + \" missing values in stalk-root column\")\n","# df_drop = dataset[dataset['stalk-root'] != \"?\"]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"cell_type":"markdown","source":["## 4 - Encode string values"],"metadata":{}},{"source":["def encode_values(dataset):\n","    mapping = {}  \n","    d = dataset.copy()\n","    labelEncoder = LabelEncoder()\n","    for column in dataset.columns:\n","        labelEncoder.fit(dataset[column])\n","        mapping[column] = dict(zip(labelEncoder.classes_, labelEncoder.transform(labelEncoder.classes_)))\n","        d[column] = labelEncoder.transform(dataset[column])\n","        \n","    return d, labelEncoder, mapping\n","\n","def print_encoding(mapping):\n","    t = PrettyTable()\n","    field_names = []\n","    rows = []\n","    for key, value in mapping.items():\n","        r = []\n","        r.append(key)\n","        for k, v in value.items():\n","            r.append(k)\n","        rows.append(r)\n","    max = []\n","    for r in rows:\n","        if len(r) > len(max):\n","            max = r\n","\n","    for r in rows:\n","        r = r + ['-'] * (len(max) - len(r))\n","        t.add_row(r)\n","    t.field_names = [\"Columns / Values\"] + list(range(0, len(max)-1))\n","    print(t)\n","\n","\n","\n","le = 0\n","pre_data, l_encoder, le_mapping = encode_values(dataset)\n","\n","# Check mapping\n","print_encoding(le_mapping)\n","\n","# Check new data\n","pre_data.head(5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"cell_type":"markdown","source":["## 5/6 - Check class and data distribution"],"metadata":{}},{"cell_type":"markdown","source":["### 5 - Check classes distribution"],"metadata":{}},{"source":["y = dataset[\"class\"].value_counts()\n","print(y)\n","class_dict = [\"edible\", \"poisonous\"]\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["# Get insights on the dataset\n","pre_data.describe()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["data = [go.Bar(\n","            x=class_dict,\n","            y=y,\n","            marker=dict(\n","            color=PLOTLY_COLORS),\n","            opacity=PLOTLY_OPACITY,\n","    )]\n","\n","layout = go.Layout(title=\"Class distribution\",\n","                   autosize=True,\n","                   yaxis=dict(\n","                        title='N. samples',\n","                    ),\n","                   )\n","fig = go.Figure(data=data, layout=layout)\n","py.iplot(fig, filename='distribution-bar')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["### 6 - Box plot"],"metadata":{}},{"source":["\n","def create_box(type, data, col, visible=False):\n","    if type == \"edible\":\n","        c = PLOTLY_COLORS[0]\n","    else:\n","        c = PLOTLY_COLORS[1]\n","    return go.Box(\n","        y = data[col],\n","        name = type,\n","        marker=dict(color = c),\n","        visible=visible,\n","        opacity=PLOTLY_OPACITY,\n","    )\n","\n","edible = pre_data[pre_data[\"class\"] == 0]\n","poisonous = pre_data[pre_data[\"class\"] == 1]\n","box_features = [col for col in pre_data.columns if ((col != 'class') and (dataset[col].nunique() > 5))]\n","\n","active_index = 0\n","box_edible = [(create_box(\"edible\", edible, col, False) if i != active_index \n","               else create_box(\"edible\", edible, col, True)) \n","              for i, col in enumerate(box_features)]\n","\n","box_poisonous = [(create_box(\"poisonous\", poisonous, col, False) if i != active_index \n","               else create_box(\"poisonous\", poisonous, col, True)) \n","              for i, col in enumerate(box_features)]\n","\n","data = box_edible + box_poisonous\n","n_features = len(box_features)\n","steps = []\n","\n","for i in range(n_features):\n","    step = dict(\n","        method = 'restyle',  \n","        args = ['visible', [False] * len(data)],\n","        label = box_features[i],\n","    )\n","    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n","    step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n","    steps.append(step)\n","    \n","sliders = [dict(\n","    active = active_index,\n","    currentvalue = dict(\n","        prefix = \"Feature: \", \n","        xanchor= 'center',\n","    ),\n","    pad = {\"t\": 50},\n","    steps = steps,\n","    len=1,\n",")]\n","\n","layout = dict(\n","    sliders=sliders,\n","    autosize=True,\n","    yaxis=dict(\n","        title='value',\n","        automargin=True,\n","    ),\n","    legend=dict(\n","        x=0,\n","        y=1,\n","    ),\n",")\n","\n","fig = dict(data=data, layout=layout)\n","py.iplot(fig, filename='box_slider')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"cell_type":"markdown","source":["### 6 - Bar graph"],"metadata":{}},{"source":["\n","def create_bar(type, data, col, visible=False):\n","    if type == \"edible\":\n","        c = PLOTLY_COLORS[0]\n","    else:\n","        c = PLOTLY_COLORS[1]\n","    return go.Histogram(\n","        x = data[col],\n","        name = type,\n","        marker=dict(color = c),\n","        visible=visible,\n","        opacity=PLOTLY_OPACITY,\n","    )\n","\n","hist_features = [col for col in pre_data.columns if (col != 'class')]\n","\n","active_index = 0\n","hist_edible = [(create_bar(\"edible\", edible, col, False) if i != active_index \n","               else create_bar(\"edible\", edible, col, True)) \n","              for i, col in enumerate(hist_features)]\n","\n","hist_poisonous = [(create_bar(\"poisonous\", poisonous, col, False) if i != active_index \n","               else create_bar(\"poisonous\", poisonous, col, True)) \n","              for i, col in enumerate(hist_features)]\n","\n","total_data = hist_edible + hist_poisonous\n","n_features = len(hist_features)\n","steps = []\n","\n","for i in range(n_features):\n","    step = dict(\n","        method = 'restyle',  \n","        args = ['visible', [False] * len(total_data)],\n","        label = hist_features[i],\n","    )\n","    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n","    step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n","    steps.append(step)\n","    \n","sliders = [dict(\n","    active = active_index,\n","    currentvalue = dict(\n","        prefix = \"Feature: \", \n","        xanchor= 'center',\n","    ),\n","    pad = {\"t\": 50},\n","    steps = steps,\n",")]\n","\n","layout = dict(\n","    sliders=sliders,\n","    autosize=True,\n","    yaxis=dict(\n","        title='value',\n","        automargin=True,\n","    ),\n","    legend=dict(\n","        x=0,\n","        y=1,\n","    ),\n","    barmode='group',\n","    bargap=0.15,\n","    bargroupgap=0.1\n",")\n","\n","fig = dict(data=total_data, layout=layout)\n","py.iplot(fig, filename='bar_slider')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"cell_type":"markdown","source":["## 7 - Correlation matrix"],"metadata":{}},{"source":["correlation_matrix = pre_data.corr(method='pearson')\n","\n","trace = go.Heatmap(\n","    z=correlation_matrix.values.tolist(), \n","    x=correlation_matrix.columns, \n","    y=correlation_matrix.columns, \n","    colorscale=COLORSCALE_HEATMAP,\n","    opacity=0.95,\n","    zmin=-1,\n","    zmax=1)\n","    \n","\n","data=[trace]\n","\n","layout = go.Layout(\n","    title='Heatmap of columns correlation',\n","    autosize=False,\n","    width=850,\n","    height=700,\n","    yaxis=go.layout.YAxis(automargin=True),\n","    xaxis=dict(tickangle=40),\n","    margin=go.layout.Margin(l=0, r=200, b=200, t=80)\n",")\n","\n","fig = go.Figure(data=data, layout=layout)\n","py.iplot(fig, filename='labelled-heatmap4')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["A dendrogram is a diagram representing a tree. This diagrammatic representation is frequently used \n","in different contexts, but we will see the case representing hierarchical clustering. \n","It illustrates the arrangement of the clusters, and its objective is to analyze if \n","we have any duplicate features.\n","In order to reduce the dimensionality of our dataset, we can identify and remove duplicate features\n","according to their pairwise correlation with others.\n","\n","The linkage criterion determines the distance between sets of observations as a function \n","of the pairwise distances between observations.\n","We will use the between-group average linkage (UPGMA). Proximity between two clusters \n","is the arithmetic mean of all the proximities between the objects of one, on one side, \n","and the objects of the other, on the other side.\n","The method is frequently set the default one in hierarhical clustering packages.\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","names = pre_data.columns\n","inverse_correlation = 1 - abs(pre_data.corr()) # This is the 'dissimilarity' method\n","\n","fig = ff.create_dendrogram(inverse_correlation.values, \n","                           labels=names, \n","                           colorscale=COLOR_PALETTE, \n","                           linkagefun=lambda x: hc.linkage(x, 'average'))\n","\n","fig['layout'].update(dict(\n","    title=\"Dendrogram of correlation among features\",\n","    width=800, \n","    height=600,\n","    xaxis=dict(\n","        title='Features',\n","    ),\n","    yaxis=dict(\n","        title='Distance',\n","        \n","    ),\n","))\n","iplot(fig, filename='dendrogram_corr_clustering')\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"cell_type":"markdown","source":["## 8/9 - Scale and divide data"],"metadata":{}},{"source":["\n","def dataframe_to_array(data):\n","    y_data = data['class']\n","    X_data = data.drop(['class'], axis=1)\n","    return X_data, y_data\n","\n","def scale_data(X_data):\n","    scaler = StandardScaler(with_mean=True, with_std=True, copy=True)\n","    return scaler.fit_transform(X_data)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","drop_data = pre_data[pre_data['stalk-root'] != le_mapping['stalk-root']['?']]\n","\n","X_pre_data, y_data = dataframe_to_array(pre_data)\n","X_scaled_data = scale_data(X_pre_data)\n","\n","X_drop_data, y_drop_data = dataframe_to_array(drop_data)\n","X_scaled_drop_data = scale_data(X_drop_data)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["# Principal component analysis"],"metadata":{}},{"source":["\n","pca = PCA(random_state=RANDOM_SEED)\n","projected_data = pca.fit_transform(X_scaled_data)\n","\n","tot_var = np.sum(pca.explained_variance_)\n","ex_var = [(i / tot_var) * 100 for i in sorted(pca.explained_variance_, reverse=True)]\n","cum_ex_var = np.cumsum(ex_var)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["cum_var_bar = go.Bar(\n","    x=list(range(1, len(cum_ex_var) + 1)), \n","    y=ex_var,\n","    name=\"Variance of each component\",\n","    marker=dict(\n","        color=PLOTLY_COLORS[0],\n","    ),\n","    opacity=PLOTLY_OPACITY\n",")\n","variance_line = go.Scatter(\n","    x=list(range(1, len(cum_ex_var) + 1)),\n","    y=cum_ex_var,\n","    mode='lines+markers',\n","    name=\"Cumulative variance\",\n","    marker=dict(\n","        color=PLOTLY_COLORS[1],\n","    ),\n","    opacity=PLOTLY_OPACITY,\n","    line=dict(\n","        shape='hv',\n","    ))\n","data = [cum_var_bar, variance_line]\n","layout = go.Layout(\n","    title='Individual and Cumulative Explained Variance',\n","    autosize=True,\n","    yaxis=dict(\n","        title='Explained variance (%)',\n","    ),\n","    xaxis=dict(\n","        title=\"Principal components\",\n","        dtick=1,\n","    ),\n","    legend=dict(\n","        x=0,\n","        y=1,\n","    ),\n",")\n","fig = go.Figure(data=data, layout=layout)\n","iplot(fig, filename='basic-bar')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["\n","n_comp = 9\n","pca.components_ = pca.components_[:n_comp]\n","reduced_data = np.dot(projected_data, pca.components_.T)\n","# pca.inverse_transform(projected_data)\n","X_df_reduced = pd.DataFrame(reduced_data, columns=[\"PC#%d\" % (x + 1) for x in range(n_comp)])\n","X_df_reduced.head(4)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["'''\n","N=pre_data.values\n","pca = PCA(n_components=2)\n","x = pca.fit_transform(N)\n","\n","kmeans = KMeans(n_clusters=2, random_state=RANDOM_SEED)\n","X_clustered = kmeans.fit_predict(N)\n","print(len(np.where(X_clustered == 0)[0]))\n","print(len(np.where(X_clustered == 1)[0]))\n","\n","ed_idx = np.where(X_clustered == 0)\n","po_idx = np.where(X_clustered == 1)\n","\n","p1 = go.Scatter(\n","    x=np.take(x[:,0], indices=ed_idx)[0],\n","    y=np.take(x[:,1], indices=ed_idx)[0],\n","    mode='markers',\n","    name=\"Edible\",\n","    marker=dict(\n","        color=PLOTLY_COLORS[0],\n","    ),\n","    opacity=PLOTLY_OPACITY)\n","\n","p2 = go.Scatter(\n","    x=np.take(x[:,0], indices=po_idx)[0],\n","    y=np.take(x[:,1], indices=po_idx)[0],\n","    mode='markers',\n","    name=\"Poisonous\",\n","    marker=dict(\n","        color=PLOTLY_COLORS[1],\n","    ),\n","    opacity=PLOTLY_OPACITY)\n","    \n","\n","data = [p1, p2]\n","\n","layout = go.Layout(\n","    title='Data clustered using first two components',\n","    autosize=True,\n","    yaxis=dict(\n","        title='Second component',\n","    ),\n","    xaxis=dict(\n","        title=\"First component\",\n","        dtick=1,\n","    ),\n","    legend=dict(\n","        x=0,\n","        y=1,\n","    ),\n",")\n","fig = go.Figure(data=data, layout=layout)\n","iplot(fig, filename='clusters-scatter')\n","'''"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"cell_type":"markdown","source":["# Classification"],"metadata":{}},{"source":["\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled_data, y_data, test_size=0.2, random_state=RANDOM_SEED)\n","X_train_pc, X_test_pc, y_train_pc, y_test_pc = train_test_split(X_df_reduced, y_data, test_size=0.2, random_state=RANDOM_SEED)\n","X_train_drop, X_test_drop, y_train_drop, y_test_drop = train_test_split(X_scaled_drop_data, y_drop_data, test_size=0.2, random_state=RANDOM_SEED)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["\n","def print_gridcv_scores(grid_search, n=5):\n","    \"\"\"\n","    Prints the best score achieved by a grid_search, alongside with its parametes\n","\n","    :param (estimator) clf: Classifier object\n","    :param (int) n: Best n scores \n","    \"\"\"    \n","\n","    if not hasattr(grid_search, 'best_score_'):\n","        raise KeyError('grid_search is not fitted.')\n","    \n","    t = PrettyTable()\n","\n","    print(\"Best grid scores on validation set:\")\n","    indexes = np.argsort(grid_search.cv_results_['mean_test_score'])[::-1][:n]\n","    means = grid_search.cv_results_['mean_test_score'][indexes]\n","    stds = grid_search.cv_results_['std_test_score'][indexes]\n","    params = np.array(grid_search.cv_results_['params'])[indexes]\n","    \n","    t.field_names = ['Score'] + [f for f in params[0].keys()] \n","    for mean, std, params in zip(means, stds, params):\n","        row=[\"%0.3f (+/-%0.03f)\" % (mean, std * 2)] + [p for p in params.values()]\n","        t.add_row(row)\n","    print(t)\n","               \n","@watcher\n","def param_tune_grid_cv(clf, params, X_train, y_train, cv):\n","    \"\"\"\n","    Function that performs a grid search over some parameters\n","\n","    :param (estimator) clf: Classifier object\n","    :param (dictionary) params: parameters to be tested in grid search\n","    :param (array-like) X_train: List of data to be trained with\n","    :param (array-like) y_train: Target relative to X for classification or regression\n","    :param (cross-validation generator) cv: Determines the cross-validation splitting strategy\n","    \"\"\"   \n","    pipeline = Pipeline([('clf', clf)])\n","    grid_search = GridSearchCV(estimator=pipeline, \n","                               param_grid=params, \n","                               cv=cv, \n","                               n_jobs=-1,       # Use all processors\n","                               scoring='f1',    # Use f1 metric for evaluation\n","                               return_train_score=True)\n","    grid_search.fit(X_train, y_train)\n","    return grid_search\n","   \n","\n","def score(clfs, datasets):\n","    \"\"\"\n","    Function that scores a classifier on some data\n","    \n","    :param (array of estimator) clf: Array of classifiers\n","    :param (dictionary) params: Dictionary of test data, passed like [(X_test, y_test)]\n","\n","    \"\"\"  \n","    scores = []\n","    for c, (X_test, y_test) in zip(clfs, datasets):\n","        scores.append(c.score(X_test, y_test))\n","\n","    return scores\n","\n","\n","def hexToRGBA(hex, alpha):\n","\n","    \"\"\"\n","    Function that returns an rgba value from an hex and an opacity value\n","    \n","    :param (String) clf: Hex value \n","    :param (float) params: Value between 0 and 1 indicating opacity\n","\n","    \"\"\"  \n","\n","    r = int(hex[1:3], 16)\n","    g = int(hex[3:5], 16)\n","    b = int(hex[5:], 16)\n","\n","    if alpha:\n","        return \"rgba(\" + str(r) + \", \" + str(g) + \", \" + str(b) + \", \" + str(alpha) + \")\"\n","    else:\n","        return \"rgb(\" + str(r) + \", \" + str(g) + \", \" + str(b) + \")\"\n","\n","\n","def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=-1, train_sizes=np.linspace(.008, 1.0, 5)):\n","    \"\"\"\n","    Generate a simple plot of the test and training learning curve.\n","\n","    Parameters\n","    ----------\n","    estimator : object type that implements the \"fit\" and \"predict\" methods\n","        An object of that type which is cloned for each validation.\n","\n","    title : string\n","        Title for the chart.\n","\n","    X : array-like, shape (n_samples, n_features)\n","        Training vector, where n_samples is the number of samples and\n","        n_features is the number of features.\n","\n","    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n","        Target relative to X for classification or regression;\n","        None for unsupervised learning.\n","\n","    cv : int, cross-validation generator or an iterable, optional\n","        Determines the cross-validation splitting strategy.\n","        Possible inputs for cv are:\n","          - None, to use the default 3-fold cross-validation,\n","          - integer, to specify the number of folds.\n","          - :term:`CV splitter`,\n","          - An iterable yielding (train, test) splits as arrays of indices.\n","\n","        For integer/None inputs, if ``y`` is binary or multiclass,\n","        :class:`StratifiedKFold` used. If the estimator is not a classifier\n","        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n","\n","        Refer :ref:`User Guide <cross_validation>` for the various\n","        cross-validators that can be used here.\n","\n","    n_jobs : int or None, optional (default=None)\n","        Number of jobs to run in parallel.\n","        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n","        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n","        for more details.\n","\n","    train_sizes : array-like, shape (n_ticks,), dtype float or int\n","        Relative or absolute numbers of training examples that will be used to\n","        generate the learning curve. If the dtype is float, it is regarded as a\n","        fraction of the maximum size of the training set (that is determined\n","        by the selected validation method), i.e. it has to be within (0, 1].\n","        Otherwise it is interpreted as absolute sizes of the training sets.\n","        Note that for classification the number of samples usually have to\n","        be big enough to contain at least one sample from each class.\n","        (default: np.linspace(0.1, 1.0, 5))\n","    \"\"\"\n","    \n","    train_sizes, train_scores, test_scores = learning_curve(\n","        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=\"f1\", random_state=RANDOM_SEED)\n","    train_scores_mean = np.mean(train_scores, axis=1)\n","    train_scores_std = np.std(train_scores, axis=1)\n","    test_scores_mean = np.mean(test_scores, axis=1)\n","    test_scores_std = np.std(test_scores, axis=1)\n","    \n","    trace1 = go.Scatter(\n","        x=train_sizes, \n","        y=train_scores_mean - train_scores_std, \n","        showlegend=False,\n","        mode=\"lines\",\n","        name=\"\",\n","        hoverlabel = dict(\n","            namelength=20\n","        ),\n","        line = dict(\n","            width = 0.1,\n","            color = hexToRGBA(PLOTLY_COLORS[0], 0.4),\n","        ),\n","    )\n","    trace2 = go.Scatter(\n","        x=train_sizes, \n","        y=train_scores_mean + train_scores_std, \n","        showlegend=False,\n","        fill=\"tonexty\",\n","        mode=\"lines\",\n","        name=\"\",\n","        hoverlabel = dict(\n","            namelength=20\n","        ),\n","        line = dict(\n","            width = 0.1,\n","            color = hexToRGBA(PLOTLY_COLORS[0], 0.4),\n","        ),\n","    )\n","    trace3 = go.Scatter(\n","        x=train_sizes, \n","        y=train_scores_mean, \n","        showlegend=True,\n","        name=\"Train score\",\n","        line = dict(\n","            color = PLOTLY_COLORS[0],\n","        ),\n","    )\n","    \n","    trace4 = go.Scatter(\n","        x=train_sizes, \n","        y=test_scores_mean - test_scores_std, \n","        showlegend=False,\n","        mode=\"lines\",\n","        name=\"\",\n","        hoverlabel = dict(\n","            namelength=20\n","        ),\n","        line = dict(\n","            width = 0.1,\n","            color = hexToRGBA(PLOTLY_COLORS[1], 0.4),\n","        ),\n","    )\n","    trace5 = go.Scatter(\n","        x=train_sizes, \n","        y=test_scores_mean + test_scores_std, \n","        showlegend=False,\n","        fill=\"tonexty\",\n","        mode=\"lines\",\n","        name=\"\",\n","        hoverlabel = dict(\n","            namelength=20\n","        ),\n","        line = dict(\n","            width = 0.1,\n","            color = hexToRGBA(PLOTLY_COLORS[1], 0.4),\n","        ),\n","    )\n","    trace6 = go.Scatter(\n","        x=train_sizes, \n","        y=test_scores_mean, \n","        showlegend=True,\n","        name=\"Test score\",\n","        line = dict(\n","            color = PLOTLY_COLORS[1],\n","        ),\n","    )\n","    \n","    data = [trace1, trace2, trace3, trace4, trace5, trace6]\n","    layout = go.Layout(\n","        title=title,\n","        autosize=True,\n","        yaxis=dict(\n","            title='F1 Score',\n","        ),\n","        xaxis=dict(\n","            title=\"#Training samples\",\n","        ),\n","        legend=dict(\n","            x=0.8,\n","            y=0,\n","        ),\n","    )\n","    fig = go.Figure(data=data, layout=layout)\n","    return iplot(fig, filename=title)\n","\n","\n","def print_confusion_matrix(gs, X_test, y_test):\n","\n","    \"\"\"\n","    Function that prints confusion matrix for a classifier\n","    \n","    :param (estimator) clf: Classifier object\n","    :param (array-like) X_test: List of data to be tested with\n","    :param (array-like) y_test: List of labels for test \n","    \"\"\"  \n","\n","    gs_score = gs.score(X_test, y_test)\n","    y_pred = gs.predict(X_test)\n","\n","    cm = confusion_matrix(y_test, y_pred)\n","    t = PrettyTable()\n","    t.add_row([\"True Edible\", cm[0][0], cm[0][1]])\n","    t.add_row([\"True Poisonous\", cm[1][0], cm[1][1]])\n","    t.field_names = [\" \", \"Predicted Edible\", \"Predicted Poisonous\"]\n","    print(t)\n","\n","    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix\n","    cm_df = pd.DataFrame(cm.round(3), index=[\"True edible\", \"True Poisonous\"], columns=[\"Predicted edible\", \"Predicted poisonous\"])\n","    cm_df\n","\n","\n","def print_raw_score(clf, X_test, y_test):\n","    \"\"\"\n","    Function that scores a classifier on some data\n","    \n","    :param (array of estimator) clf: Array of classifiers\n","    :param (array-like) X_test: List of data to be tested with\n","    :param (array-like) y_test: List of labels for test \n","\n","    \"\"\"  \n","    print(\"Score achieved by NB: %0.3f\" % (score([clf], [(X_test, y_test)])[0]))\n","\n","\n","def plot_feature_importance(feature_importance, title):\n","    \"\"\"\n","    Function that plots feature importance for a decision tree or a random forest classifier\n","    \n","    :param (dictionary) feature_importance: Dictionary of most important features sorted\n","    :param (str) title: Title of the plot\n","\n","    \"\"\" \n","    \n","    trace1 = go.Bar(\n","        x=feature_importance[:, 0],\n","        y=feature_importance[:, 1],\n","        marker = dict(color = PLOTLY_COLORS[0]),\n","        opacity=PLOTLY_OPACITY,\n","        name='Feature importance'\n","    )\n","    data = [trace1]\n","    layout = go.Layout(\n","        title=title,\n","        autosize=True,\n","        margin=go.layout.Margin(l=50, r=100, b=150),\n","        xaxis=dict(\n","            title='feature',\n","            tickangle=30\n","        ),\n","        yaxis=dict(\n","            title='feature importance',\n","            automargin=True,\n","        ),\n","    )\n","    fig = go.Figure(data=data, layout=layout)\n","    return iplot(fig, filename=title)\n","\n","\n","def print_performances(classifiers, classifier_names, auc_scores, X_test, y_test):\n","  \n","    \"\"\"\n","    Function that scores a classifier on some data\n","    \n","    :param (array of estimator) clf: Array of classifiers\n","    :param (array-like) classifier_names: Title of the classifier\n","    :param (array-like) auc-score: Auc scores\n","    :param (array-like) X_test: List of data to be tested with\n","    :param (array-like) y_test: List of labels for test \n","\n","    \"\"\" \n","\n","    accs = []\n","    recalls = []\n","    precision = []\n","    results_table = pd.DataFrame(columns=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"auc\"])\n","    for (i, clf), name, auc in zip(enumerate(classifiers), classifier_names, auc_scores):\n","        y_pred = clf.predict(X_test)\n","        row = []\n","        row.append(accuracy_score(y_test, y_pred))\n","        row.append(precision_score(y_test, y_pred))\n","        row.append(recall_score(y_test, y_pred))\n","        row.append(f1_score(y_test, y_pred))\n","        row.append(auc)\n","        row = [\"%.3f\" % r for r in row]\n","        results_table.loc[name] = row\n","    return results_table\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["kf = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED)\n","clf_lr = LogisticRegression(random_state=RANDOM_SEED)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["\n","print(\"Full dataset cv:\")\n","gs_full = param_tune_grid_cv(clf_lr, LOGISTIC_REGRESSION_PARAMS, X_train, y_train, kf)\n","print(\"\\nDataset projected on first 9 pc cv:\")\n","gs_pc = param_tune_grid_cv(clf_lr, LOGISTIC_REGRESSION_PARAMS, X_train_pc, y_train_pc, kf)\n","print(\"\\nFull dataset with dropped values took:\")\n","gs_drop = param_tune_grid_cv(clf_lr, LOGISTIC_REGRESSION_PARAMS, X_train_drop, y_train_drop, kf)\n","gss = [gs_full, gs_pc, gs_drop]\n","\n","test_results = score(gss, [(X_test, y_test), (X_test_pc, y_test_pc), (X_test_drop, y_test_drop)])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["'''\n","print(\"Full dataset cv:\")\n","gs_full_balanced = param_tune_grid_cv(clf_lr_balanced, LOGISTIC_REGRESSION_PARAMS, X_train, y_train, kf)\n","print(\"\\nDataset projected on first 9 pc cv:\")\n","gs_pc_balanced = param_tune_grid_cv(clf_lr_balanced, LOGISTIC_REGRESSION_PARAMS, X_train_pc, y_train_pc, kf)\n","print(\"\\nFull dataset with dropped values took:\")\n","gs_drop_balanced = param_tune_grid_cv(clf_lr_balanced, LOGISTIC_REGRESSION_PARAMS, X_train_drop, y_train_drop, kf)\n","gss_balanced = [gs_full_balanced, gs_pc_balanced, gs_drop_balanced]\n","\n","test_results_balanced = score(gss_balanced, [(X_test, y_test), (X_test_pc, y_test_pc), (X_test_drop, y_test_drop)])\n","'''"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["X_train.shape\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["dataset_strings = [\"full dataset\", \"dataset with first 9 principal components\", \"dataset with dropped missing values\"]\n","method_strings = [\"without any balancing\"]\n","\n","t = PrettyTable()\n","t.field_names = [\"Score\", \"Dataset\", \"Type\"]\n","\n","result_row = []\n","for ms, results in zip(method_strings, [test_results, test_results_balanced]):\n","    for ds, res in zip(dataset_strings, results):\n","        result_row.append([\"%.3f\" % res, ds, ms])\n","        \n","result_row = sorted(result_row, key=lambda kv: kv[0], reverse=True)\n","\n","for k in result_row:\n","    t.add_row(k)\n","\n","t.title = \"F1 score  dataset and method\"\n","print(t)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["print_gridcv_scores(gs_drop)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print_confusion_matrix(gs_drop, X_test_drop, y_test_drop)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plot_learning_curve(gs_drop.best_estimator_, \"Learning Curve of Logistic Regression\", \n","                    np.concatenate((X_train_drop, X_test_drop)),\n","                    np.concatenate((y_train_drop, y_test_drop)), \n","                    cv=5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":["## Support vector machine"],"metadata":{}},{"source":["clf_svm = SVC(probability=True, random_state=RANDOM_SEED)\n","gs_pc_svm = param_tune_grid_cv(clf_svm, SVM_PARAMS, X_train_pc, y_train_pc, kf)\n","print_gridcv_scores(gs_pc_svm, n=5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plot_learning_curve(gs_pc_svm.best_estimator_, \"Learning curve of SVM\", \n","                    np.concatenate((X_train_pc, X_test_pc)),\n","                    np.concatenate((y_train_pc, y_test_pc)),\n","                    cv=5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print_confusion_matrix(gs_pc_svm, X_test_pc, y_test_pc)"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["clf_nb = GaussianNB()\n","clf_nb.fit(X_train, y_train)\n","print_raw_score(clf_nb, X_test, y_test)\n","print_confusion_matrix(clf_nb, X_test, y_test)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plot_learning_curve(clf_nb, \"Learning curve of GaussianNB\", \n","                    np.concatenate((X_train, X_test), axis=0), \n","                    np.concatenate((y_train, y_test), axis=0), \n","                    cv=5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","clf_pc_rf = RandomForestClassifier(random_state=RANDOM_SEED)\n","gs_pc_rf = param_tune_grid_cv(clf_pc_rf, RANDOM_FOREST_PARAMS, X_train_pc, y_train_pc, kf)\n","print_gridcv_scores(gs_pc_rf, n = 5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print_confusion_matrix(gs_pc_rf, X_test_pc, y_test_pc)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plot_learning_curve(gs_pc_rf.best_estimator_, \"Learning curve of Random Forest Classifier\", \n","                    np.concatenate((X_train_pc, X_test_pc)),\n","                    np.concatenate((y_train_pc, y_test_pc)), \n","                    cv=5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["feature_importance = np.array(  sorted(zip(X_train_pc.columns, \n","                                gs_pc_rf.best_estimator_.named_steps['clf'].feature_importances_),\n","                                key=lambda x: x[1], reverse=True))\n","plot_feature_importance(feature_importance, \"Feature importance in the random forest\")\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["'''\n","print(\"Full dataset cv:\")\n","gs_full = param_tune_grid_cv(clf_svm, SVM_PARAMS, X_train, y_train, kf)\n","print(\"\\nDataset projected on first 9 pc cv:\")\n","gs_pc = param_tune_grid_cv(clf_svm, SVM_PARAMS, X_train_pc, y_train_pc, kf)\n","print(\"\\nFull dataset with dropped values took:\")\n","gs_drop = param_tune_grid_cv(clf_svm, SVM_PARAMS, X_train_drop, y_train_drop, kf)\n","gss = [gs_full, gs_pc, gs_drop]\n","\n","test_results = score(gss, [(X_test, y_test), (X_test_pc, y_test_pc), (X_test_drop, y_test_drop)])\n","'''"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","clf_knn = KNeighborsClassifier()\n","gs_knn = param_tune_grid_cv(clf_knn, KNN_PARAMS, X_train_pc, y_train_pc, kf)\n","print_gridcv_scores(gs_knn, n=5)\n","\n","'''\n","clf_knn = KNeighborsClassifier()\n","gs_knn = param_tune_grid_cv(clf_knn, KNN_PARAMS, X_train, y_train, kf)\n","print_gridcv_scores(gs_knn, n=5)\n","'''"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print_confusion_matrix(gs_knn, X_train_pc, y_train_pc)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","plot_learning_curve(gs_knn.best_estimator_, \"Learning curve of Random Forest Classifier\", \n","                    np.concatenate((X_train_pc, X_test_pc)),\n","                    np.concatenate((y_train_pc, y_test_pc)), \n","                    cv=5)\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["\n","def plot_roc_curve(classifiers, legend, title, X_test, y_test):\n","    t1 = go.Scatter(\n","        x=[0, 1], \n","        y=[0, 1], \n","        showlegend=False,\n","        mode=\"lines\",\n","        name=\"\",\n","        line = dict(\n","            color = COLOR_PALETTE[0],\n","        ),\n","    )\n","    \n","    data = [t1]\n","    aucs = []\n","    for clf, string, c in zip(classifiers, legend, COLOR_PALETTE[1:]):\n","        y_test_roc = np.array([([0, 1] if y else [1, 0]) for y in y_test])\n","        y_score = clf.predict_proba(X_test)\n","        \n","        # Compute ROC curve and ROC area for each class\n","        fpr = dict()\n","        tpr = dict()\n","        roc_auc = dict()\n","        for i in range(2):\n","            fpr[i], tpr[i], _ = roc_curve(y_test_roc[:, i], y_score[:, i])\n","            roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","        # Compute micro-average ROC curve and ROC area\n","        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_roc.ravel(), y_score.ravel())\n","        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","        aucs.append(roc_auc['micro'])\n","\n","        trace = go.Scatter(\n","            x=fpr['micro'], \n","            y=tpr['micro'], \n","            showlegend=True,\n","            mode=\"lines\",\n","            name=string + \" (area = %0.2f)\" % roc_auc['micro'],\n","            hoverlabel = dict(\n","                namelength=30\n","            ),\n","            line = dict(\n","                color = c,\n","            ),\n","        )\n","        data.append(trace)\n","\n","    layout = go.Layout(\n","        title=title,\n","        autosize=False,\n","        width=550,\n","        height=550,\n","        yaxis=dict(\n","            title='True Positive Rate',\n","        ),\n","        xaxis=dict(\n","            title=\"False Positive Rate\",\n","        ),\n","        legend=dict(\n","            x=0.4,\n","            y=0.06,\n","        ),\n","    )\n","    fig = go.Figure(data=data, layout=layout)\n","    return aucs, iplot(fig, filename=title)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","classifiers = [gs_drop, gs_pc_svm, clf_nb, gs_pc_rf, gs_knn]\n","classifier_names = [\"Logistic Regression\", \"SVM\", \"GaussianNB\", \"Random Forest\", \"KNN\"]\n","auc_scores, roc_plot = plot_roc_curve(classifiers, classifier_names, \"ROC curve\", X_test, y_test)\n","roc_plot\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print_performances(classifiers, classifier_names, auc_scores, X_test_pc, y_test_pc)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}},{"source":["pre_data.columns\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["data_vis = pre_data.drop(['odor', 'spore-print-color'], axis=1)\n","data_vis = data_vis[data_vis['stalk-root'] != le_mapping['stalk-root']['?']]\n","\n","data_vis.shape\n","\n","X_data_vis, y_data_vis = dataframe_to_array(data_vis)\n","X_data_vis = scale_data(X_data_vis)\n","\n","pca = PCA(random_state=RANDOM_SEED)\n","proj_data = pca.fit_transform(X_data_vis)\n","tot_var = np.sum(pca.explained_variance_)\n","ex_var = [(i / tot_var) * 100 for i in sorted(pca.explained_variance_, reverse=True)]\n","cum_ex_var = np.cumsum(ex_var)\n","n_comp = 9\n","pca.components_ = pca.components_[:n_comp]\n","reduced_data = np.dot(proj_data, pca.components_.T)\n","# pca.inverse_transform(projected_data)\n","X_vis_reduced = pd.DataFrame(reduced_data, columns=[\"PC#%d\" % (x + 1) for x in range(n_comp)])\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["X_train_vis, X_test_vis, y_train_vis, y_test_vis = train_test_split(X_vis_reduced, y_data_vis, test_size=0.2, random_state=RANDOM_SEED)\n","\n","clf_svm = SVC(probability=True, random_state=RANDOM_SEED)\n","gs_pc_svm = param_tune_grid_cv(clf_svm, SVM_PARAMS, X_train_vis, y_train_vis, kf)\n","print_gridcv_scores(gs_pc_svm, n=5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["plot_learning_curve(gs_pc_svm.best_estimator_, \"Learning curve of SVM\", \n","                    np.concatenate((X_train_vis, X_test_vis)),\n","                    np.concatenate((y_train_vis, y_test_vis)),\n","                    cv=5)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["print_confusion_matrix(gs_pc_svm, X_test_vis, y_test_vis)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}